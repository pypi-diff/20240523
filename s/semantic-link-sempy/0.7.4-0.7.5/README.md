# Comparing `tmp/semantic_link_sempy-0.7.4-py3-none-any.whl.zip` & `tmp/semantic_link_sempy-0.7.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,103 +1,103 @@
-Zip file size: 2966270 bytes, number of entries: 101
--rw-rw-r--  2.0 unx     1097 b- defN 24-May-15 07:52 sempy/__init__.py
--rw-rw-r--  2.0 unx      497 b- defN 24-May-15 07:56 sempy/_version.py
--rw-rw-r--  2.0 unx      167 b- defN 24-May-15 07:52 sempy/dotnet.runtime.config.json
--rw-rw-r--  2.0 unx        0 b- defN 24-May-15 07:52 sempy/_metadata/__init__.py
--rw-rw-r--  2.0 unx    11141 b- defN 24-May-15 07:52 sempy/_metadata/_mdataframe.py
--rw-rw-r--  2.0 unx      828 b- defN 24-May-15 07:52 sempy/_metadata/_meta_utils.py
--rw-rw-r--  2.0 unx     3017 b- defN 24-May-15 07:52 sempy/_metadata/_mseries.py
--rw-rw-r--  2.0 unx        0 b- defN 24-May-15 07:52 sempy/_utils/__init__.py
--rw-rw-r--  2.0 unx    17194 b- defN 24-May-15 07:52 sempy/_utils/_log.py
--rw-rw-r--  2.0 unx     1180 b- defN 24-May-15 07:52 sempy/_utils/_ordered_set.py
--rw-rw-r--  2.0 unx     6195 b- defN 24-May-15 07:52 sempy/_utils/_pandas_utils.py
--rw-rw-r--  2.0 unx      109 b- defN 24-May-15 07:52 sempy/dependencies/__init__.py
--rw-rw-r--  2.0 unx    10216 b- defN 24-May-15 07:52 sempy/dependencies/_find.py
--rw-rw-r--  2.0 unx     3897 b- defN 24-May-15 07:52 sempy/dependencies/_plot.py
--rw-rw-r--  2.0 unx     6023 b- defN 24-May-15 07:52 sempy/dependencies/_stats.py
--rw-rw-r--  2.0 unx     4181 b- defN 24-May-15 07:52 sempy/dependencies/_validate.py
--rw-rw-r--  2.0 unx     3543 b- defN 24-May-15 07:52 sempy/fabric/__init__.py
--rw-rw-r--  2.0 unx      959 b- defN 24-May-15 07:52 sempy/fabric/_cache.py
--rw-rw-r--  2.0 unx      495 b- defN 24-May-15 07:52 sempy/fabric/_datacategory.py
--rw-rw-r--  2.0 unx     1520 b- defN 24-May-15 07:52 sempy/fabric/_daxmagics.py
--rw-rw-r--  2.0 unx     4946 b- defN 24-May-15 07:52 sempy/fabric/_environment.py
--rw-rw-r--  2.0 unx    56530 b- defN 24-May-15 07:52 sempy/fabric/_flat.py
--rw-rw-r--  2.0 unx     3727 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_annotations.py
--rw-rw-r--  2.0 unx      992 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_apps.py
--rw-rw-r--  2.0 unx     2932 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_calculation_items.py
--rw-rw-r--  2.0 unx    14677 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_columns.py
--rw-rw-r--  2.0 unx      835 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_dataflows.py
--rw-rw-r--  2.0 unx     3110 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_datasources.py
--rw-rw-r--  2.0 unx      765 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_gateways.py
--rw-rw-r--  2.0 unx     5072 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_hierarchies.py
--rw-rw-r--  2.0 unx     7697 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_partitions.py
--rw-rw-r--  2.0 unx     2218 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_perspectives.py
--rw-rw-r--  2.0 unx     7317 b- defN 24-May-15 07:52 sempy/fabric/_flat_list_relationships.py
--rw-rw-r--  2.0 unx     1256 b- defN 24-May-15 07:52 sempy/fabric/_metadatakeys.py
--rw-rw-r--  2.0 unx     2945 b- defN 24-May-15 07:52 sempy/fabric/_token_provider.py
--rw-rw-r--  2.0 unx     9111 b- defN 24-May-15 07:52 sempy/fabric/_utils.py
--rw-rw-r--  2.0 unx      384 b- defN 24-May-15 07:52 sempy/fabric/_client/__init__.py
--rw-rw-r--  2.0 unx     2715 b- defN 24-May-15 07:52 sempy/fabric/_client/_adomd_connection.py
--rw-rw-r--  2.0 unx    28046 b- defN 24-May-15 07:52 sempy/fabric/_client/_base_dataset_client.py
--rw-rw-r--  2.0 unx      639 b- defN 24-May-15 07:52 sempy/fabric/_client/_connection_mode.py
--rw-rw-r--  2.0 unx     1541 b- defN 24-May-15 07:52 sempy/fabric/_client/_dataset_onelake_import.py
--rw-rw-r--  2.0 unx     9814 b- defN 24-May-15 07:52 sempy/fabric/_client/_dataset_rest_client.py
--rw-rw-r--  2.0 unx    13483 b- defN 24-May-15 07:52 sempy/fabric/_client/_dataset_xmla_client.py
--rw-rw-r--  2.0 unx     7719 b- defN 24-May-15 07:52 sempy/fabric/_client/_fabric_rest_api.py
--rw-rw-r--  2.0 unx    14867 b- defN 24-May-15 07:52 sempy/fabric/_client/_pbi_rest_api.py
--rw-rw-r--  2.0 unx     1673 b- defN 24-May-15 07:52 sempy/fabric/_client/_refresh_execution_details.py
--rw-rw-r--  2.0 unx     9301 b- defN 24-May-15 07:52 sempy/fabric/_client/_rest_client.py
--rw-rw-r--  2.0 unx     3681 b- defN 24-May-15 07:52 sempy/fabric/_client/_tools.py
--rw-rw-r--  2.0 unx     2906 b- defN 24-May-15 07:52 sempy/fabric/_client/_utils.py
--rw-rw-r--  2.0 unx    25368 b- defN 24-May-15 07:52 sempy/fabric/_client/_workspace_client.py
--rw-rw-r--  2.0 unx        0 b- defN 24-May-15 07:52 sempy/fabric/_dataframe/__init__.py
--rw-rw-r--  2.0 unx    27600 b- defN 24-May-15 07:52 sempy/fabric/_dataframe/_fabric_dataframe.py
--rw-rw-r--  2.0 unx     1764 b- defN 24-May-15 07:52 sempy/fabric/_dataframe/_fabric_series.py
--rw-rw-r--  2.0 unx        0 b- defN 24-May-15 07:52 sempy/fabric/_trace/__init__.py
--rw-rw-r--  2.0 unx    11613 b- defN 24-May-15 07:52 sempy/fabric/_trace/_trace.py
--rw-rw-r--  2.0 unx     7110 b- defN 24-May-15 07:52 sempy/fabric/_trace/_trace_connection.py
--rw-rw-r--  2.0 unx      288 b- defN 24-May-15 07:52 sempy/fabric/exceptions/__init__.py
--rw-rw-r--  2.0 unx     2705 b- defN 24-May-15 07:52 sempy/fabric/exceptions/_exceptions.py
--rw-rw-r--  2.0 unx      503 b- defN 24-May-15 07:52 sempy/fabric/matcher/__init__.py
--rw-rw-r--  2.0 unx     3701 b- defN 24-May-15 07:52 sempy/fabric/matcher/_matcher.py
--rw-rw-r--  2.0 unx      462 b- defN 24-May-15 07:52 sempy/functions/__init__.py
--rw-rw-r--  2.0 unx     6093 b- defN 24-May-15 07:52 sempy/functions/_decorator.py
--rw-rw-r--  2.0 unx     6226 b- defN 24-May-15 07:52 sempy/functions/_function.py
--rw-rw-r--  2.0 unx     6444 b- defN 24-May-15 07:52 sempy/functions/_matcher_dataframe.py
--rw-rw-r--  2.0 unx     1602 b- defN 24-May-15 07:52 sempy/functions/_matcher_series.py
--rw-rw-r--  2.0 unx     4810 b- defN 24-May-15 07:52 sempy/functions/_registry.py
--rw-rw-r--  2.0 unx     2156 b- defN 24-May-15 07:52 sempy/functions/_util.py
--rw-rw-r--  2.0 unx        0 b- defN 24-May-15 07:52 sempy/functions/_dataframe/__init__.py
--rw-rw-r--  2.0 unx     1192 b- defN 24-May-15 07:52 sempy/functions/_dataframe/_sdataframe.py
--rw-rw-r--  2.0 unx     1188 b- defN 24-May-15 07:52 sempy/functions/_dataframe/_sseries.py
--rw-rw-r--  2.0 unx      335 b- defN 24-May-15 07:52 sempy/functions/matcher/__init__.py
--rw-rw-r--  2.0 unx     5880 b- defN 24-May-15 07:52 sempy/functions/matcher/_matcher.py
+Zip file size: 2968104 bytes, number of entries: 101
+-rw-rw-r--  2.0 unx     1097 b- defN 24-May-23 15:16 sempy/__init__.py
+-rw-rw-r--  2.0 unx      497 b- defN 24-May-23 15:21 sempy/_version.py
+-rw-rw-r--  2.0 unx      167 b- defN 24-May-23 15:16 sempy/dotnet.runtime.config.json
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-23 15:16 sempy/_metadata/__init__.py
+-rw-rw-r--  2.0 unx    11114 b- defN 24-May-23 15:16 sempy/_metadata/_mdataframe.py
+-rw-rw-r--  2.0 unx      828 b- defN 24-May-23 15:16 sempy/_metadata/_meta_utils.py
+-rw-rw-r--  2.0 unx     3011 b- defN 24-May-23 15:16 sempy/_metadata/_mseries.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-23 15:16 sempy/_utils/__init__.py
+-rw-rw-r--  2.0 unx    17194 b- defN 24-May-23 15:16 sempy/_utils/_log.py
+-rw-rw-r--  2.0 unx     1180 b- defN 24-May-23 15:16 sempy/_utils/_ordered_set.py
+-rw-rw-r--  2.0 unx     6205 b- defN 24-May-23 15:16 sempy/_utils/_pandas_utils.py
+-rw-rw-r--  2.0 unx      109 b- defN 24-May-23 15:16 sempy/dependencies/__init__.py
+-rw-rw-r--  2.0 unx    10216 b- defN 24-May-23 15:16 sempy/dependencies/_find.py
+-rw-rw-r--  2.0 unx     3897 b- defN 24-May-23 15:16 sempy/dependencies/_plot.py
+-rw-rw-r--  2.0 unx     6023 b- defN 24-May-23 15:16 sempy/dependencies/_stats.py
+-rw-rw-r--  2.0 unx     4197 b- defN 24-May-23 15:16 sempy/dependencies/_validate.py
+-rw-rw-r--  2.0 unx     3635 b- defN 24-May-23 15:16 sempy/fabric/__init__.py
+-rw-rw-r--  2.0 unx      959 b- defN 24-May-23 15:16 sempy/fabric/_cache.py
+-rw-rw-r--  2.0 unx      495 b- defN 24-May-23 15:16 sempy/fabric/_datacategory.py
+-rw-rw-r--  2.0 unx     1520 b- defN 24-May-23 15:16 sempy/fabric/_daxmagics.py
+-rw-rw-r--  2.0 unx     4946 b- defN 24-May-23 15:16 sempy/fabric/_environment.py
+-rw-rw-r--  2.0 unx    58829 b- defN 24-May-23 15:16 sempy/fabric/_flat.py
+-rw-rw-r--  2.0 unx     3727 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_annotations.py
+-rw-rw-r--  2.0 unx      992 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_apps.py
+-rw-rw-r--  2.0 unx     2932 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_calculation_items.py
+-rw-rw-r--  2.0 unx    14677 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_columns.py
+-rw-rw-r--  2.0 unx      835 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_dataflows.py
+-rw-rw-r--  2.0 unx     3110 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_datasources.py
+-rw-rw-r--  2.0 unx      765 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_gateways.py
+-rw-rw-r--  2.0 unx     5072 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_hierarchies.py
+-rw-rw-r--  2.0 unx     7697 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_partitions.py
+-rw-rw-r--  2.0 unx     2218 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_perspectives.py
+-rw-rw-r--  2.0 unx     7317 b- defN 24-May-23 15:16 sempy/fabric/_flat_list_relationships.py
+-rw-rw-r--  2.0 unx     1256 b- defN 24-May-23 15:16 sempy/fabric/_metadatakeys.py
+-rw-rw-r--  2.0 unx     2945 b- defN 24-May-23 15:16 sempy/fabric/_token_provider.py
+-rw-rw-r--  2.0 unx     9166 b- defN 24-May-23 15:16 sempy/fabric/_utils.py
+-rw-rw-r--  2.0 unx      384 b- defN 24-May-23 15:16 sempy/fabric/_client/__init__.py
+-rw-rw-r--  2.0 unx     2715 b- defN 24-May-23 15:16 sempy/fabric/_client/_adomd_connection.py
+-rw-rw-r--  2.0 unx    28045 b- defN 24-May-23 15:16 sempy/fabric/_client/_base_dataset_client.py
+-rw-rw-r--  2.0 unx      639 b- defN 24-May-23 15:16 sempy/fabric/_client/_connection_mode.py
+-rw-rw-r--  2.0 unx     1541 b- defN 24-May-23 15:16 sempy/fabric/_client/_dataset_onelake_import.py
+-rw-rw-r--  2.0 unx     9814 b- defN 24-May-23 15:16 sempy/fabric/_client/_dataset_rest_client.py
+-rw-rw-r--  2.0 unx    13483 b- defN 24-May-23 15:16 sempy/fabric/_client/_dataset_xmla_client.py
+-rw-rw-r--  2.0 unx     6128 b- defN 24-May-23 15:16 sempy/fabric/_client/_fabric_rest_api.py
+-rw-rw-r--  2.0 unx    14867 b- defN 24-May-23 15:16 sempy/fabric/_client/_pbi_rest_api.py
+-rw-rw-r--  2.0 unx     1673 b- defN 24-May-23 15:16 sempy/fabric/_client/_refresh_execution_details.py
+-rw-rw-r--  2.0 unx    14804 b- defN 24-May-23 15:16 sempy/fabric/_client/_rest_client.py
+-rw-rw-r--  2.0 unx     3681 b- defN 24-May-23 15:16 sempy/fabric/_client/_tools.py
+-rw-rw-r--  2.0 unx     2906 b- defN 24-May-23 15:16 sempy/fabric/_client/_utils.py
+-rw-rw-r--  2.0 unx    24772 b- defN 24-May-23 15:16 sempy/fabric/_client/_workspace_client.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-23 15:16 sempy/fabric/_dataframe/__init__.py
+-rw-rw-r--  2.0 unx    27732 b- defN 24-May-23 15:16 sempy/fabric/_dataframe/_fabric_dataframe.py
+-rw-rw-r--  2.0 unx     1764 b- defN 24-May-23 15:16 sempy/fabric/_dataframe/_fabric_series.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-23 15:16 sempy/fabric/_trace/__init__.py
+-rw-rw-r--  2.0 unx    11613 b- defN 24-May-23 15:16 sempy/fabric/_trace/_trace.py
+-rw-rw-r--  2.0 unx     7126 b- defN 24-May-23 15:16 sempy/fabric/_trace/_trace_connection.py
+-rw-rw-r--  2.0 unx      288 b- defN 24-May-23 15:16 sempy/fabric/exceptions/__init__.py
+-rw-rw-r--  2.0 unx     2705 b- defN 24-May-23 15:16 sempy/fabric/exceptions/_exceptions.py
+-rw-rw-r--  2.0 unx      503 b- defN 24-May-23 15:16 sempy/fabric/matcher/__init__.py
+-rw-rw-r--  2.0 unx     3701 b- defN 24-May-23 15:16 sempy/fabric/matcher/_matcher.py
+-rw-rw-r--  2.0 unx      462 b- defN 24-May-23 15:16 sempy/functions/__init__.py
+-rw-rw-r--  2.0 unx     6093 b- defN 24-May-23 15:16 sempy/functions/_decorator.py
+-rw-rw-r--  2.0 unx     6290 b- defN 24-May-23 15:16 sempy/functions/_function.py
+-rw-rw-r--  2.0 unx     6492 b- defN 24-May-23 15:16 sempy/functions/_matcher_dataframe.py
+-rw-rw-r--  2.0 unx     1602 b- defN 24-May-23 15:16 sempy/functions/_matcher_series.py
+-rw-rw-r--  2.0 unx     4810 b- defN 24-May-23 15:16 sempy/functions/_registry.py
+-rw-rw-r--  2.0 unx     2156 b- defN 24-May-23 15:16 sempy/functions/_util.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-23 15:16 sempy/functions/_dataframe/__init__.py
+-rw-rw-r--  2.0 unx     1192 b- defN 24-May-23 15:16 sempy/functions/_dataframe/_sdataframe.py
+-rw-rw-r--  2.0 unx     1188 b- defN 24-May-23 15:16 sempy/functions/_dataframe/_sseries.py
+-rw-rw-r--  2.0 unx      335 b- defN 24-May-23 15:16 sempy/functions/matcher/__init__.py
+-rw-rw-r--  2.0 unx     5896 b- defN 24-May-23 15:16 sempy/functions/matcher/_matcher.py
 -rwxr--r--  2.0 unx   170496 b- defN 20-Oct-13 00:20 sempy/lib/Apache.Arrow.dll
 -rwxr--r--  2.0 unx    13312 b- defN 23-May-10 17:41 sempy/lib/IronCompress.dll
 -rwxr--r--  2.0 unx   846392 b- defN 23-Nov-02 21:14 sempy/lib/Microsoft.AnalysisServices.AdomdClient.dll
 -rwxr--r--  2.0 unx  1146416 b- defN 23-Nov-02 21:15 sempy/lib/Microsoft.AnalysisServices.Core.dll
 -rwxr--r--  2.0 unx    98864 b- defN 23-Nov-02 21:14 sempy/lib/Microsoft.AnalysisServices.Runtime.Core.dll
 -rwxr--r--  2.0 unx    96816 b- defN 23-Nov-02 21:14 sempy/lib/Microsoft.AnalysisServices.Runtime.Windows.dll
 -rwxr--r--  2.0 unx   563248 b- defN 23-Nov-02 21:15 sempy/lib/Microsoft.AnalysisServices.Tabular.Json.dll
 -rwxr--r--  2.0 unx  1640496 b- defN 23-Nov-02 21:15 sempy/lib/Microsoft.AnalysisServices.Tabular.dll
 -rwxr--r--  2.0 unx   675888 b- defN 23-Nov-02 21:15 sempy/lib/Microsoft.AnalysisServices.dll
 -rwxr--r--  2.0 unx   597632 b- defN 23-Jan-31 23:19 sempy/lib/Microsoft.Data.Analysis.dll
--rw-rw-r--  2.0 unx    16896 b- defN 24-May-15 07:53 sempy/lib/Microsoft.Fabric.SemanticLink.XmlaTools.dll
+-rw-rw-r--  2.0 unx    16896 b- defN 24-May-23 15:18 sempy/lib/Microsoft.Fabric.SemanticLink.XmlaTools.dll
 -rwxr--r--  2.0 unx    64960 b- defN 23-Mar-02 23:04 sempy/lib/Microsoft.IO.RecyclableMemoryStream.dll
 -rwxr--r--  2.0 unx  1402840 b- defN 22-Apr-04 18:15 sempy/lib/Microsoft.Identity.Client.dll
 -rwxr--r--  2.0 unx    48256 b- defN 23-Jan-31 23:19 sempy/lib/Microsoft.ML.DataView.dll
 -rwxr--r--  2.0 unx   692736 b- defN 23-Jun-30 10:39 sempy/lib/Parquet.dll
 -rwxr--r--  2.0 unx    41472 b- defN 23-Mar-27 11:54 sempy/lib/Snappier.dll
 -rwxr--r--  2.0 unx   442368 b- defN 23-Apr-06 12:25 sempy/lib/ZstdSharp.dll
--rw-rw-r--  2.0 unx      383 b- defN 24-May-15 07:52 sempy/relationships/__init__.py
--rw-rw-r--  2.0 unx    14768 b- defN 24-May-15 07:52 sempy/relationships/_find.py
--rw-rw-r--  2.0 unx      278 b- defN 24-May-15 07:52 sempy/relationships/_multiplicity.py
--rw-rw-r--  2.0 unx     5548 b- defN 24-May-15 07:52 sempy/relationships/_plot.py
--rw-rw-r--  2.0 unx     5665 b- defN 24-May-15 07:52 sempy/relationships/_stats.py
--rw-rw-r--  2.0 unx     3209 b- defN 24-May-15 07:52 sempy/relationships/_utils.py
--rw-rw-r--  2.0 unx     5934 b- defN 24-May-15 07:52 sempy/relationships/_validate.py
--rw-rw-r--  2.0 unx    12690 b- defN 24-May-15 07:56 semantic_link_sempy-0.7.4.dist-info/LICENSE.txt
--rw-rw-r--  2.0 unx     4972 b- defN 24-May-15 07:56 semantic_link_sempy-0.7.4.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 24-May-15 07:56 semantic_link_sempy-0.7.4.dist-info/WHEEL
--rw-rw-r--  2.0 unx        6 b- defN 24-May-15 07:56 semantic_link_sempy-0.7.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     9196 b- defN 24-May-15 07:56 semantic_link_sempy-0.7.4.dist-info/RECORD
-101 files, 9032027 bytes uncompressed, 2951598 bytes compressed:  67.3%
+-rw-rw-r--  2.0 unx      383 b- defN 24-May-23 15:16 sempy/relationships/__init__.py
+-rw-rw-r--  2.0 unx    14778 b- defN 24-May-23 15:16 sempy/relationships/_find.py
+-rw-rw-r--  2.0 unx      278 b- defN 24-May-23 15:16 sempy/relationships/_multiplicity.py
+-rw-rw-r--  2.0 unx     5548 b- defN 24-May-23 15:16 sempy/relationships/_plot.py
+-rw-rw-r--  2.0 unx     5697 b- defN 24-May-23 15:16 sempy/relationships/_stats.py
+-rw-rw-r--  2.0 unx     3219 b- defN 24-May-23 15:16 sempy/relationships/_utils.py
+-rw-rw-r--  2.0 unx     5934 b- defN 24-May-23 15:16 sempy/relationships/_validate.py
+-rw-rw-r--  2.0 unx    12690 b- defN 24-May-23 15:21 semantic_link_sempy-0.7.5.dist-info/LICENSE.txt
+-rw-rw-r--  2.0 unx     6862 b- defN 24-May-23 15:21 semantic_link_sempy-0.7.5.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 24-May-23 15:21 semantic_link_sempy-0.7.5.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        6 b- defN 24-May-23 15:21 semantic_link_sempy-0.7.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     9197 b- defN 24-May-23 15:21 semantic_link_sempy-0.7.5.dist-info/RECORD
+101 files, 9040000 bytes uncompressed, 2953432 bytes compressed:  67.3%
```

## zipnote {}

```diff
@@ -282,23 +282,23 @@
 
 Filename: sempy/relationships/_utils.py
 Comment: 
 
 Filename: sempy/relationships/_validate.py
 Comment: 
 
-Filename: semantic_link_sempy-0.7.4.dist-info/LICENSE.txt
+Filename: semantic_link_sempy-0.7.5.dist-info/LICENSE.txt
 Comment: 
 
-Filename: semantic_link_sempy-0.7.4.dist-info/METADATA
+Filename: semantic_link_sempy-0.7.5.dist-info/METADATA
 Comment: 
 
-Filename: semantic_link_sempy-0.7.4.dist-info/WHEEL
+Filename: semantic_link_sempy-0.7.5.dist-info/WHEEL
 Comment: 
 
-Filename: semantic_link_sempy-0.7.4.dist-info/top_level.txt
+Filename: semantic_link_sempy-0.7.5.dist-info/top_level.txt
 Comment: 
 
-Filename: semantic_link_sempy-0.7.4.dist-info/RECORD
+Filename: semantic_link_sempy-0.7.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sempy/_version.py

```diff
@@ -4,18 +4,18 @@
 # unpacked source archive. Distribution tarballs contain a pre-generated copy
 # of this file.
 
 import json
 
 version_json = '''
 {
- "date": "2024-05-15T06:41:06+0000",
+ "date": "2024-05-23T13:57:54+0000",
  "dirty": false,
  "error": null,
- "full-revisionid": "189a9b530e3fc3f119f4396eafae23273984959f",
- "version": "0.7.4"
+ "full-revisionid": "45c8326a694b5ea9e6117d4ba01fe3a9c006e7ea",
+ "version": "0.7.5"
 }
 '''  # END VERSION_JSON
 
 
 def get_versions():
     return json.loads(version_json)
```

## sempy/_metadata/_mdataframe.py

```diff
@@ -1,12 +1,11 @@
 import pandas as pd
 import inspect
 import warnings
 
-from pandas.core.generic import NDFrame
 from numpy import ndarray
 from typing import Any, Callable, Dict, Iterable, List, Optional, Union
 
 
 class MDataFrame(pd.DataFrame):
     """
     An extension of a Pandas DataFrame that allows storage and propogation of column metadata.
@@ -39,51 +38,51 @@
     def __init__(
         self,
         data: Optional[Union[ndarray, Iterable, dict, pd.DataFrame]] = None,
         *args: Any,
         column_metadata: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ):
-        super().__init__(data, *args, **kwargs)
+        super().__init__(data, *args, **kwargs)  # type: ignore
         if column_metadata:
             self.column_metadata = column_metadata
 
-    def __finalize__(self, other, method: Optional[str] = None, **kwargs) -> NDFrame:
+    def __finalize__(self, other, method: Optional[str] = None, **kwargs) -> 'MDataFrame':
         """
         Override pandas __finalize__ to propagate metadata from other to self
 
         This is the only method of propagation for "General functions" such as
         pandas.concat([df1, df2]), pandas.merge(df1, df2) that are not executed on an object instance.
         Overriding the DataFrame method is therefore not an option.
         """
-        self = super().__finalize__(other, method=method, **kwargs)
+        self = super().__finalize__(other, method=method, **kwargs)  # type: ignore
 
         # dump_meta(self, other, method)
 
         column_metadata = self.column_metadata
 
         # pandas implements a large number of methods with "merge" and "concat" and slicing.
         # The physical implementation can be verified by placing a debugger breakpoint here.
         if method == "merge":
-            column_metadata = _merge_metadata(other.left, other.right, self._headers, other.suffixes)
+            column_metadata = _merge_metadata(other.left, other.right, other.suffixes)
         elif method == "concat":
             column_metadata = _concat_metadata(other.objs, self._headers)
 
         self._column_metadata = column_metadata
         self._trim_metadata_to_columns()
 
         return self
 
-    def to_parquet(self, path: str, *args, **kwargs) -> None:
+    def to_parquet(self, path: Any, *args, **kwargs) -> None:  # type: ignore
         """
         Write the DataFrame including metadata to a parquet file specified by path parameter using Arrow.
 
         Parameters
         ----------
-        path : str
+        path : Any
             String containing the filepath to where the parquet should be saved.
         *args : Any
             Other args to be passed to PyArrow ``write_table``.
         **kwargs : Any
             Other kwargs to be passed to PyArrow ``write_table``.
         """
         import json
@@ -215,15 +214,15 @@
                 if value:
                     renamed_metadata[self._headers[i]] = value
             self._column_metadata = renamed_metadata
         else:
             self._column_metadata
 
 
-def _merge_metadata(left: pd.DataFrame, right: pd.DataFrame, result_columns: pd.Index, suffixes: tuple) -> dict:
+def _merge_metadata(left: pd.DataFrame, right: pd.DataFrame, suffixes: tuple) -> dict:
     merged_metadata = {}
 
     # The choice of logic here is tricky, since it must account for columns that originally
     # ended with one of the suffixes, even before the merge. The suffix cannot be used
     # for determinging the origin of the column. Suffix is applied only in case of conflict.
     def _merge_for_suffix(df, other, suffix):
         column_metadata = getattr(df, "column_metadata", None)
@@ -242,15 +241,15 @@
     # precedence, so we process "left" after right, so that it overrides.
     _merge_for_suffix(right, left, suffixes[1])
     _merge_for_suffix(left, right, suffixes[0])
 
     return merged_metadata
 
 
-def _concat_metadata(objs: list, columns: pd.Index) -> dict:
+def _concat_metadata(objs: list, columns: List[str]) -> dict:
     merged_metadata = {}
     for name in columns:
         metadata_candidates = [
             o.column_metadata[name]
             for o in objs
             if _column_metadata_has(name, o)
         ]
```

## sempy/_metadata/_mseries.py

```diff
@@ -1,9 +1,8 @@
 import pandas as pd
-from pandas.core.generic import NDFrame
 from typing import Any, Dict, Optional, Callable
 
 
 class MSeries(pd.Series):
     """
     An extension of a Pandas Series that allows storage and propogation of column metadata.
 
@@ -26,21 +25,21 @@
     def __init__(
         self,
         data=None,
         *args,
         column_metadata: Optional[Dict[str, Any]] = None,
         **kwargs
     ):
-        super().__init__(data, *args, **kwargs)
+        super().__init__(data, *args, **kwargs)  # type: ignore
         if column_metadata:
             self.column_metadata = column_metadata
 
-    def __finalize__(self, other, method: Optional[str] = None, **kwargs) -> NDFrame:
+    def __finalize__(self, other, method: Optional[str] = None, **kwargs) -> 'MSeries':
         """Override pandas __finalize__ to propagate metadata from other to self"""
-        self = super().__finalize__(other, method=method, **kwargs)
+        self = super().__finalize__(other, method=method, **kwargs)  # type: ignore
 
         if self.column_metadata and self.name in self.column_metadata:
             self.column_metadata = {self.name: self.column_metadata[self.name]}
 
         return self
 
     @property
```

## sempy/_utils/_pandas_utils.py

```diff
@@ -85,15 +85,15 @@
         second=int(second),
         microsecond=int(ms_or_ns),
         tzinfo=datetime.timezone.utc)
 
     return dt
 
 
-def rename_and_validate(df: pd.DataFrame, schema: List[Tuple[str, str, str]]) -> pd.DataFrame:
+def rename_and_validate(df: Optional[pd.DataFrame], schema: List[Tuple[str, str, str]]) -> pd.DataFrame:
     """
     Rename columns in a dataframe according to a schema and validate that the schema is satisfied.
 
     Parameters
     ----------
     df : pd.DataFrame
         The input data.
```

## sempy/dependencies/_validate.py

```diff
@@ -13,15 +13,15 @@
         max_violations: int = 10000,
         order_by: str = "count"
 ) -> pd.DataFrame:
     order_by_options = ["count", "determinant"]
     if order_by not in order_by_options:
         raise ValueError(f"Unexpected order_by argument given. Must be in {order_by_options}")
 
-    pairs = df.groupby([determinant, dependent], dropna=dropna).size().reset_index(name='count')
+    pairs = df.groupby([determinant, dependent], dropna=dropna).size().reset_index(name='count')  # type: ignore
     # only keep the rows with duplicated values of A (violations)
     violations_df = pairs[pairs.duplicated(determinant, keep=False)]
 
     if show_feeding_determinants:
         violations_df = pairs.merge(violations_df[[dependent]].drop_duplicates(), how="inner")
 
     if order_by == "count":
```

## sempy/fabric/__init__.py

```diff
@@ -26,14 +26,16 @@
     list_workspaces,
     plot_relationships,
     read_table,
     refresh_dataset,
     refresh_tom_cache,
     resolve_workspace_id,
     resolve_workspace_name,
+    resolve_item_id,
+    resolve_item_name,
     run_notebook_job,
     _trace_evaluate_dax
 )
 from sempy.fabric._flat_list_annotations import list_annotations
 from sempy.fabric._flat_list_apps import list_apps
 from sempy.fabric._flat_list_calculation_items import list_calculation_items
 from sempy.fabric._flat_list_columns import list_columns
@@ -108,10 +110,12 @@
     "plot_relationships",
     "read_parquet",
     "read_table",
     "refresh_dataset",
     "refresh_tom_cache",
     "resolve_workspace_id",
     "resolve_workspace_name",
+    "resolve_item_id",
+    "resolve_item_name",
     "run_notebook_job",
     "_trace_evaluate_dax"
 ]
```

## sempy/fabric/_flat.py

```diff
@@ -900,16 +900,16 @@
 
     Returns
     -------
     graphviz.Digraph
         Graph object containing all relationships.
         If include_attributes is true, attributes are represented as ports in the graph.
     """
-    named_dataframes = _to_dataframe_dict(tables)
-    relationships = _get_relationships(named_dataframes)
+    named_dataframes = _to_dataframe_dict(tables)  # type: ignore
+    relationships = _get_relationships(named_dataframes)  # type: ignore
     return plot_relationship_metadata(
         relationships,
         tables,
         include_columns=include_columns,
         missing_key_errors=missing_key_errors,
         graph_attributes=graph_attributes)
 
@@ -947,16 +947,16 @@
 
     Returns
     -------
     pandas.DataFrame
         Dataframe with relationships, error type and error message.
         If there are no violations, returns an empty DataFrame.
     """
-    named_dataframes = _to_dataframe_dict(tables)
-    relationships = _get_relationships(named_dataframes)
+    named_dataframes = _to_dataframe_dict(tables)  # type: ignore
+    relationships = _get_relationships(named_dataframes)  # type: ignore
     return _list_relationship_violations(named_dataframes, relationships, missing_key_errors, coverage_threshold, n_keys)
 
 
 @log
 def resolve_workspace_id(workspace: Optional[Union[str, UUID]] = None) -> str:
     """
     Resolve the workspace name or ID to the workspace UUID.
@@ -994,14 +994,74 @@
         The workspace name.
     """
 
     return _get_or_create_workspace_client(workspace).get_workspace_name()
 
 
 @log
+def resolve_item_id(item_name: str, type: Optional[str] = None,
+                    workspace: Optional[Union[str, UUID]] = None) -> str:
+    """
+    Resolve the item ID by name in the specified workspace.
+
+    The item type can be given to limit the search. Otherwise the function will search for all items in the workspace.
+
+    Please see `ItemTypes <https://learn.microsoft.com/en-us/rest/api/fabric/core/items/create-item?tabs=HTTP#itemtype>_`
+    for all supported item types.
+
+    Parameters
+    ----------
+    item_name : str
+        Name of the item to be resolved.
+    type : str, default = None
+        Type of the item to be resolved.
+    workspace : str or uuid.UUID, default=None
+        The Fabric workspace name or UUID object containing the workspace ID. Defaults to None
+        which resolves to the workspace of the attached lakehouse
+        or if no lakehouse attached, resolves to the workspace of the notebook.
+
+    Returns
+    -------
+    str
+        The item ID of the specified item.
+    """
+    return _get_or_create_workspace_client(workspace).resolve_item_id(item_name, type=type)
+
+
+@log
+def resolve_item_name(item_id: UUID, type: Optional[str] = None,
+                      workspace: Optional[Union[str, UUID]] = None) -> str:
+    """
+    Resolve the item name by ID in the specified workspace.
+
+    The item type can be given to limit the search. Otherwise the function will search for all items in the workspace.
+
+    Please see `ItemTypes <https://learn.microsoft.com/en-us/rest/api/fabric/core/items/create-item?tabs=HTTP#itemtype>_`
+    for all supported item types.
+
+    Parameters
+    ----------
+    item_id : str
+        ID of the item to be resolved.
+    type : str, default = None
+        Type of the item to be resolved.
+    workspace : str or uuid.UUID, default=None
+        The Fabric workspace name or UUID object containing the workspace ID. Defaults to None
+        which resolves to the workspace of the attached lakehouse
+        or if no lakehouse attached, resolves to the workspace of the notebook.
+
+    Returns
+    -------
+    str
+        The item ID of the specified item.
+    """
+    return _get_or_create_workspace_client(workspace).resolve_item_name(item_id, type=type)
+
+
+@log
 def create_trace_connection(
     dataset: Union[str, UUID],
     workspace: Optional[Union[str, UUID]] = None
 ) -> TraceConnection:
     """
     Create a TraceConnection to the server specified by the dataset.
```

## sempy/fabric/_flat_list_partitions.py

```diff
@@ -145,11 +145,11 @@
                                  workspace=workspace)
 
         df_stats = pd.merge(df_stats, df_id_map, on='SemPyPartitionStorageID', how='left')
         df_stats = pd.merge(df_stats, df_table_parts, on='SemPyPartitionID', how='left')
 
         df = pd.merge(df, df_stats, how='left', left_on=["Table Name", "Partition Name"], right_on=["SemPyTableName", "SemPyPartitionName"])
 
-        df.drop({'SemPyPartitionStorageID', 'SemPyPartitionID', 'SemPyTableID', 'SemPyPartitionName', 'SemPyTableName'}, axis=1, inplace=True)
+        df.drop(['SemPyPartitionStorageID', 'SemPyPartitionID', 'SemPyTableID', 'SemPyPartitionName', 'SemPyTableName'], axis=1, inplace=True)
         df.rename({'SemPyRecordCount': 'Record Count', 'SemPySegmentCount': 'Segment Count', 'SemPyRecordsPerSegment': 'Records per Segment'}, axis=1, inplace=True)
 
     return df
```

## sempy/fabric/_utils.py

```diff
@@ -7,14 +7,15 @@
 
 from sempy.relationships._multiplicity import Multiplicity
 
 from typing import Any, Callable, Dict, Iterable, List, Tuple, Optional, Union, TYPE_CHECKING
 
 if TYPE_CHECKING:
     from sempy.fabric import FabricDataFrame
+    from pandas._libs import NaTType
 
 
 def _get_relationships(named_dataframes: Dict[str, "FabricDataFrame"]) -> pd.DataFrame:
 
     from sempy.fabric import FabricDataFrame
 
     relationship_tuples: List[Tuple] = []
@@ -71,15 +72,15 @@
         return self._dotnet_date
 
 
 _dotnet_pandas_min_date = LazyDotNetDate(pd.Timestamp.min)
 _dotnet_pandas_max_date = LazyDotNetDate(pd.Timestamp.max)
 
 
-def dotnet_to_pandas_date(dt, milliseconds=False) -> datetime.datetime:
+def dotnet_to_pandas_date(dt, milliseconds=False) -> Union[datetime.datetime, 'NaTType']:
     # convert NaN to NaT
     if pd.isna(dt):
         return pd.NaT
 
     # catch date issues early (e.g. dt.ToString() can be "1-01-01 00:00:00" which is not parsable by Pandas)
     if dt < _dotnet_pandas_min_date.dotnet_date() or dt > _dotnet_pandas_max_date.dotnet_date():
         return pd.NaT
```

## sempy/fabric/_client/_base_dataset_client.py

```diff
@@ -374,15 +374,15 @@
                             self._populate_relationship_meta(relationship, meta_df, exclude_internal)
                 if multiindex_hierarchies:
                     meta_df = self._convert_hierarchies(meta_df, table, fully_qualified_columns)
                 return meta_df
 
         raise ValueError(f"'{table_name}' is not a valid table in Dataset '{self.resolver.dataset_name}'")
 
-    def resolve_metadata(self, columns: List[str], verbose: int = 0) -> Dict[str, Any]:
+    def resolve_metadata(self, columns: pd.Index, verbose: int = 0) -> Dict[str, Any]:
         """
         Resolve column names to their Power BI metadata.
 
         Parameters
         ----------
         columns : list of str
             List of column names to resolve. Column names can be in any of the following formats:
```

## sempy/fabric/_client/_fabric_rest_api.py

```diff
@@ -1,211 +1,158 @@
-import base64
-import json
-from urllib.parse import quote
-from dataclasses import dataclass
-
-from sempy.fabric._client._rest_client import FabricRestClient
-from sempy.fabric.exceptions import FabricHTTPException
-from sempy.fabric._token_provider import TokenProvider
-from typing import Optional, Union
-
-
-@dataclass
-class OperationStatus:
-    status: str
-    retry_after: int
-    percent_complete: int
-
-
-@dataclass
-class OperationStart:
-    operation_id: str
-    retry_after: int
-
-
-@dataclass
-class JobStatus:
-    status: str
-    retry_after: int
-
-
-class _FabricRestAPI():
-    _rest_client: FabricRestClient
-
-    def __init__(self, token_provider: Optional[TokenProvider] = None):
-        self._rest_client = FabricRestClient(token_provider)
-
-    def get_my_workspace_id(self) -> str:
-        # TODO: we should align on a single API to retrieve workspaces using a single API,
-        #       but we need to wait until the API support filtering and paging
-        # Using new Fabric REST endpoints
-        payload = self.list_workspaces()
-
-        workspaces = [ws for ws in payload if ws["type"] == 'Personal']
-
-        if len(workspaces) != 1:
-            raise ValueError(f"Unable to resolve My workspace ID. Zero or more than one workspaces found ({len(workspaces)})")
-
-        return workspaces[0]['id']
-
-    def create_workspace(self, display_name: str, capacity_id: Optional[str] = None, description: Optional[str] = None) -> str:
-        payload = {"displayName": display_name}
-
-        if capacity_id is not None:
-            payload["capacityId"] = capacity_id
-
-        if description is not None:
-            payload["description"] = description
-
-        response = self._rest_client.post("v1/workspaces", json=payload)
-        if response.status_code != 201:
-            raise FabricHTTPException(response)
-
-        return response.json()["id"]
-
-    def delete_workspace(self, workspace_id: str):
-        response = self._rest_client.delete(f"v1/workspaces/{workspace_id}")
-        if response.status_code != 200:
-            raise FabricHTTPException(response)
-
-    def create_item(self, workspace_id: str, payload) -> Union[str, OperationStart]:
-        path = f"v1/workspaces/{workspace_id}/items"
-
-        response = self._rest_client.post(path,
-                                          json=payload,
-                                          headers={'Content-Type': 'application/json'})
-
-        if response.status_code == 201:
-            return response.json()["id"]
-
-        if response.status_code == 202:
-            return OperationStart(response.headers["x-ms-operation-id"],
-                                  int(response.headers.get("Retry-After", 2)))
-
-        raise FabricHTTPException(response)
-
-    def create_lakehouse(self, workspace_id: str, display_name: str, description: Optional[str] = None) -> Union[str, OperationStart]:
-        payload = {
-            "displayName": display_name,
-            "type": "Lakehouse"
-        }
-
-        if description is not None:
-            payload["description"] = description
-
-        return self.create_item(workspace_id, payload)
-
-    def delete_item(self, workspace_id: str, artifact_id: str):
-        path = f"v1/workspaces/{workspace_id}/items/{artifact_id}"
-
-        response = self._rest_client.delete(path)
-
-        if response.status_code != 200:
-            raise FabricHTTPException(response)
-
-    def create_notebook(self, workspace_id: str, display_name: str, description: Optional[str] = None, content: Optional[str] = None) -> Union[str, OperationStart]:
-        payload: dict[str, Union[str, dict]] = {
-            "displayName": display_name,
-            "type": "Notebook"
-        }
-
-        if description is not None:
-            payload["description"] = description
-
-        if content is not None:
-            payload["definition"] = {
-                "format": "ipynb",
-                "parts": [
-                    {
-                        "path": "artifact.content.ipynb",
-                        "payload": base64.b64encode(content.encode("utf-8")).decode("utf-8"),
-                        "payloadType": "InlineBase64"
-                    }
-                ]
-            }
-
-        return self.create_item(workspace_id, payload)
-
-    def get_operation_status(self, operation_id: str) -> OperationStatus:
-        response = self._rest_client.get(
-            f"v1/operations/{operation_id}",
-            headers={'Content-Type': 'application/json'}
-        )
-        response_json = response.json()
-
-        return OperationStatus(
-            response_json['status'],
-            int(response.headers.get("Retry-After", 2)),
-            response_json["percentComplete"])
-
-    def get_operation_result(self, operation_id: str) -> str:
-        response = self._rest_client.get(
-            f"v1/operations/{operation_id}/result",
-            headers={'Content-Type': 'application/json'}
-        )
-        if response.status_code != 200:
-            raise RuntimeError(f"Error Response: {response.json()}")
-
-        # TODO: public api gives json in utf-8-sig encoding, not ascii
-        return json.loads(response.content.decode('utf-8-sig'))['id']
-
-    def run_item_job(self, workspace_id: str, item_id: str, jobType: str, executionData: Optional[dict] = None) -> OperationStart:
-        response = self._rest_client.post(
-            f"v1/workspaces/{workspace_id}/items/{item_id}/jobs/instances?jobType={jobType}",
-            data=json.dumps({"executionData": {}},),
-            headers={'Content-Type': 'application/json'}
-        )
-        if response.status_code != 202:
-            raise FabricHTTPException(response)
-
-        return OperationStart(response.headers['Location'].split('/')[-1],
-                              int(response.headers.get("Retry-After", 2)))
-
-    def run_notebook_job(self, workspace_id: str, notebook_id: str) -> OperationStart:
-        return self.run_item_job(workspace_id, notebook_id, "RunNotebook")
-
-    def get_job_status(self, workspace_id: str, item_id: str, run_id: str) -> JobStatus:
-        response = self._rest_client.get(
-            f"v1/workspaces/{workspace_id}/items/{item_id}/jobs/instances/{run_id}",
-            headers={'Content-Type': 'application/json'})
-
-        if response.status_code != 200:
-            raise FabricHTTPException(response)
-
-        return JobStatus(response.json()['status'],
-                         response.headers.get("Retry-After", 2))
-
-    def _list_with_paging(self, path: str) -> list:
-        headers = {'Content-Type': 'application/json'}
-        response = self._rest_client.get(path, headers=headers)
-
-        if response.status_code != 200:
-            raise RuntimeError(f"Listing failed. Response {response.status_code}: {response.reason}, {response.text}")
-
-        rows = []
-        while True:
-            response_json = response.json()
-            rows.extend(response_json['value'])
-
-            continuation_uri = response_json.get("continuationUri")
-            if continuation_uri is None:
-                break
-
-            response = self._rest_client.get(continuation_uri, headers=headers)
-            if response.status_code != 200:
-                raise RuntimeError(f"Listing failed. Response {response.status_code}: {response.reason}, {response.text}")
-
-        return rows
-
-    def list_items(self, workspace_id: str, type: Optional[str] = None) -> list:
-        path = f"v1/workspaces/{workspace_id}/items"
-
-        if type is not None:
-            path += f"?type={quote(type)}"
-
-        return self._list_with_paging(path)
-
-    def list_workspaces(self) -> list:
-        return self._list_with_paging("v1/workspaces")
-
-    def list_capacities(self):
-        return self._list_with_paging("v1/capacities")
+import base64
+import json
+from urllib.parse import quote
+from dataclasses import dataclass
+
+from sempy.fabric._client._rest_client import FabricRestClient, OperationStart
+from sempy.fabric.exceptions import FabricHTTPException
+from sempy.fabric._token_provider import TokenProvider
+from typing import Optional, Union
+
+
+@dataclass
+class JobStatus:
+    status: str
+    retry_after: int
+
+
+class _FabricRestAPI():
+    _rest_client: FabricRestClient
+
+    def __init__(self, token_provider: Optional[TokenProvider] = None):
+        self._rest_client = FabricRestClient(token_provider)
+
+    def get_my_workspace_id(self) -> str:
+        # TODO: we should align on a single API to retrieve workspaces using a single API,
+        #       but we need to wait until the API support filtering and paging
+        # Using new Fabric REST endpoints
+        payload = self.list_workspaces()
+
+        workspaces = [ws for ws in payload if ws["type"] == 'Personal']
+
+        if len(workspaces) != 1:
+            raise ValueError(f"Unable to resolve My workspace ID. Zero or more than one workspaces found ({len(workspaces)})")
+
+        return workspaces[0]['id']
+
+    def create_workspace(self, display_name: str, capacity_id: Optional[str] = None, description: Optional[str] = None) -> str:
+        payload = {"displayName": display_name}
+
+        if capacity_id is not None:
+            payload["capacityId"] = capacity_id
+
+        if description is not None:
+            payload["description"] = description
+
+        response = self._rest_client.post("v1/workspaces", json=payload)
+        if response.status_code != 201:
+            raise FabricHTTPException(response)
+
+        return response.json()["id"]
+
+    def delete_workspace(self, workspace_id: str):
+        response = self._rest_client.delete(f"v1/workspaces/{workspace_id}")
+        if response.status_code != 200:
+            raise FabricHTTPException(response)
+
+    def create_item(self, workspace_id: str, payload, lro_max_attempts: int, lro_operation_name: str) -> str:
+        path = f"v1/workspaces/{workspace_id}/items"
+
+        response = self._rest_client.post(path,
+                                          json=payload,
+                                          headers={'Content-Type': 'application/json'},
+                                          lro_wait=True,
+                                          lro_max_attempts=lro_max_attempts,
+                                          lro_operation_name=lro_operation_name)
+
+        if response.status_code in [200, 201]:
+            return response.json()["id"]
+        else:
+            raise FabricHTTPException(response)
+
+    def create_lakehouse(self,
+                         workspace_id: str,
+                         display_name: str,
+                         description: Optional[str] = None,
+                         lro_max_attempts: int = 10) -> str:
+        payload = {
+            "displayName": display_name,
+            "type": "Lakehouse"
+        }
+
+        if description is not None:
+            payload["description"] = description
+
+        return self.create_item(workspace_id, payload, lro_max_attempts, "create lakehouse")
+
+    def delete_item(self, workspace_id: str, artifact_id: str):
+        path = f"v1/workspaces/{workspace_id}/items/{artifact_id}"
+
+        response = self._rest_client.delete(path)
+
+        if response.status_code != 200:
+            raise FabricHTTPException(response)
+
+    def create_notebook(self,
+                        workspace_id: str,
+                        display_name: str,
+                        description: Optional[str] = None,
+                        content: Optional[str] = None,
+                        lro_max_attempts: int = 10) -> str:
+        payload: dict[str, Union[str, dict]] = {
+            "displayName": display_name,
+            "type": "Notebook"
+        }
+
+        if description is not None:
+            payload["description"] = description
+
+        if content is not None:
+            payload["definition"] = {
+                "format": "ipynb",
+                "parts": [
+                    {
+                        "path": "artifact.content.ipynb",
+                        "payload": base64.b64encode(content.encode("utf-8")).decode("utf-8"),
+                        "payloadType": "InlineBase64"
+                    }
+                ]
+            }
+
+        return self.create_item(workspace_id, payload, lro_max_attempts, "create notebook")
+
+    def run_item_job(self, workspace_id: str, item_id: str, jobType: str, executionData: Optional[dict] = None) -> OperationStart:
+        response = self._rest_client.post(
+            f"v1/workspaces/{workspace_id}/items/{item_id}/jobs/instances?jobType={jobType}",
+            data=json.dumps({"executionData": {}},),
+            headers={'Content-Type': 'application/json'}
+        )
+
+        return OperationStart(response)
+
+    def run_notebook_job(self, workspace_id: str, notebook_id: str) -> OperationStart:
+        return self.run_item_job(workspace_id, notebook_id, "RunNotebook")
+
+    def get_job_status(self, workspace_id: str, item_id: str, run_id: str) -> JobStatus:
+        response = self._rest_client.get(
+            f"v1/workspaces/{workspace_id}/items/{item_id}/jobs/instances/{run_id}",
+            headers={'Content-Type': 'application/json'})
+
+        if response.status_code != 200:
+            raise FabricHTTPException(response)
+
+        return JobStatus(response.json()['status'],
+                         response.headers.get("Retry-After", 2))
+
+    def list_items(self, workspace_id: str, type: Optional[str] = None) -> list:
+        path = f"v1/workspaces/{workspace_id}/items"
+
+        if type is not None:
+            path += f"?type={quote(type)}"
+
+        return self._rest_client.get_paged(path)
+
+    def list_workspaces(self) -> list:
+        return self._rest_client.get_paged("v1/workspaces")
+
+    def list_capacities(self):
+        return self._rest_client.get_paged("v1/capacities")
```

## sempy/fabric/_client/_rest_client.py

```diff
@@ -1,275 +1,421 @@
-import uuid
-from abc import ABC, abstractmethod
-
-from sempy.fabric.exceptions import FabricHTTPException
-from requests.adapters import HTTPAdapter, Retry
-from requests.sessions import Session
-from sempy._utils._log import log_retry, log_rest_response, log_rest_request
-from sempy.fabric._token_provider import SynapseTokenProvider, TokenProvider
-
-from sempy.fabric._environment import _get_synapse_endpoint, _get_environment, _get_fabric_rest_endpoint
-from typing import Optional
-
-
-class RetryWithLogging(Retry):
-    @log_retry
-    def increment(self, *args, **kwargs):
-        return super().increment(*args, **kwargs)
-
-
-class SessionWithLogging(Session):
-    @log_rest_request
-    def prepare_request(self, *args, **kwargs):
-        return super().prepare_request(*args, **kwargs)
-
-
-class BaseRestClient(ABC):
-    """
-    REST client to access Fabric and PowerBI endpoints. Authentication tokens are automatically acquired from the execution environment.
-
-    ***Experimental***: This class is experimental and may change in future versions.
-
-    Parameters
-    ----------
-    token_provider : TokenProvider, default=None
-        Implementation of TokenProvider that can provide auth token
-        for access to the PowerBI workspace. Will attempt to acquire token
-        from its execution environment if not provided.
-    """
-    def __init__(self, token_provider: Optional[TokenProvider] = None):
-        self.http = SessionWithLogging()
-
-        @log_rest_response
-        def validate_rest_response(response, *args, **kwargs):
-            if response.status_code >= 400:
-                raise FabricHTTPException(response)
-        self.http.hooks["response"] = [validate_rest_response]
-        retry_strategy = RetryWithLogging(
-            total=10,
-            allowed_methods=["HEAD", "GET", "POST", "PUT", "PATCH", "DELETE"],
-            status_forcelist=[429, 500, 502, 503, 504],
-            backoff_factor=1
-        )
-        retry_adapter = HTTPAdapter(max_retries=retry_strategy)
-        self.http.mount("https://", retry_adapter)
-
-        self.token_provider = token_provider or SynapseTokenProvider()
-        self.default_base_url = self._get_default_base_url()
-
-    @abstractmethod
-    def _get_default_base_url(self):
-        pass
-
-    def _get_headers(self) -> dict:
-        # this could be static / a function
-        correlation_id = str(uuid.uuid4())
-        return {
-            'authorization': f'Bearer {self.token_provider()}',
-            'Accept': 'application/json',
-            'ActivityId': correlation_id
-        }
-
-    def request(self, method: str, path_or_url: str, *args, **kwargs):
-        """
-        Request to the Fabric and PowerBI REST API.
-
-        Parameters
-        ----------
-        method : str
-            HTTP method.
-        path_or_url : str
-            The path or the url to the resource.
-        *args : list
-            Arguments passed to the request method.
-        **kwargs : dict
-            Arguments passed to the request method.
-
-        Returns
-        -------
-        requests.Response
-            The response from the REST API.
-        """
-        headers = self._get_headers()
-        headers.update(kwargs.get("headers", {}))
-
-        # overwrite url + headers
-        if path_or_url.startswith("https://"):
-            url = path_or_url
-        else:
-            url = f"{self.default_base_url}{path_or_url}"
-
-        kwargs["url"] = url
-        kwargs["headers"] = headers
-
-        return self.http.request(method, *args, **kwargs)
-
-    def get(self, path_or_url: str, *args, **kwargs):
-        """
-        GET request to the Fabric and PowerBI REST API.
-
-        Parameters
-        ----------
-        path_or_url : str
-            The relative path to the resource or the full url.
-            If it's relative, the base URL is automatically prepended.
-        *args : list
-            Arguments passed to the request method.
-        **kwargs : dict
-            Arguments passed to the request method.
-
-        Returns
-        -------
-        requests.Response
-            The response from the REST API.
-        """
-        return self.request("GET", path_or_url, *args, **kwargs)
-
-    def post(self, path_or_url: str, *args, **kwargs):
-        """
-        POST request to the Fabric and PowerBI REST API.
-
-        Parameters
-        ----------
-        path_or_url : str
-            The relative path to the resource or the full url.
-            If it's relative, the base URL is automatically prepended.
-        *args : list
-            Arguments passed to the request method.
-        **kwargs : dict
-            Arguments passed to the request method.
-
-        Returns
-        -------
-        requests.Response
-            The response from the REST API.
-        """
-        return self.request("POST", path_or_url, *args, **kwargs)
-
-    def delete(self, path_or_url: str, *args, **kwargs):
-        """
-        DELETE request to the Fabric and PowerBI REST API.
-
-        Parameters
-        ----------
-        path_or_url : str
-            The relative path to the resource or the full url.
-            If it's relative, the base URL is automatically prepended.
-        *args : list
-            Arguments passed to the request method.
-        **kwargs : dict
-            Arguments passed to the request method.
-
-        Returns
-        -------
-        requests.Response
-            The response from the REST API.
-        """
-        return self.request("DELETE", path_or_url, *args, **kwargs)
-
-    def head(self, path_or_url: str, *args, **kwargs):
-        """
-        HEAD request to the Fabric and PowerBI REST API.
-
-        Parameters
-        ----------
-        path_or_url : str
-            The relative path to the resource or the full url.
-            If it's relative, the base URL is automatically prepended.
-        *args : list
-            Arguments passed to the request method.
-        **kwargs : dict
-            Arguments passed to the request method.
-
-        Returns
-        -------
-        requests.Response
-            The response from the REST API.
-        """
-        return self.request("HEAD", path_or_url, *args, **kwargs)
-
-    def patch(self, path_or_url: str, *args, **kwargs):
-        """
-        PATCH request to the Fabric and PowerBI REST API.
-
-        Parameters
-        ----------
-        path_or_url : str
-            The relative path to the resource or the full url.
-            If it's relative, the base URL is automatically prepended.
-        *args : list
-            Arguments passed to the request method.
-        **kwargs : dict
-            Arguments passed to the request method.
-
-        Returns
-        -------
-        requests.Response
-            The response from the REST API.
-        """
-        return self.request("PATCH", path_or_url, *args, **kwargs)
-
-    def put(self, path_or_url: str, *args, **kwargs):
-        """
-        PUT request to the Fabric and PowerBI REST API.
-
-        Parameters
-        ----------
-        path_or_url : str
-            The relative path to the resource or the full url.
-            If it's relative, the base URL is automatically prepended.
-        *args : list
-            Arguments passed to the request method.
-        **kwargs : dict
-            Arguments passed to the request method.
-
-        Returns
-        -------
-        requests.Response
-            The response from the REST API.
-        """
-        return self.request("PUT", path_or_url, *args, **kwargs)
-
-
-class FabricRestClient(BaseRestClient):
-    """
-    REST client to access Fabric REST endpoints. Authentication tokens are automatically acquired from the execution environment.
-
-    ***Experimental***: This class is experimental and may change in future versions.
-
-    Parameters
-    ----------
-    token_provider : TokenProvider, default=None
-        Implementation of TokenProvider that can provide auth token
-        for access to the PowerBI workspace. Will attempt to acquire token
-        from its execution environment if not provided.
-    """
-    def __init__(self, token_provider: Optional[TokenProvider] = None):
-        super().__init__(token_provider)
-
-    def _get_default_base_url(self):
-        return _get_fabric_rest_endpoint()
-
-
-class PowerBIRestClient(BaseRestClient):
-    """
-    REST client to access PowerBI REST endpoints. Authentication tokens are automatically acquired from the execution environment.
-
-    ***Experimental***: This class is experimental and may change in future versions.
-
-    Parameters
-    ----------
-    token_provider : TokenProvider, default=None
-        Implementation of TokenProvider that can provide auth token
-        for access to the PowerBI workspace. Will attempt to acquire token
-        from its execution environment if not provided.
-    """
-    def __init__(self, token_provider: Optional[TokenProvider] = None):
-        super().__init__(token_provider)
-
-    def _get_default_base_url(self):
-        # The endpoint api.powerbi.com does not work for REST calls using the "pbi" token due to limited audience
-        if _get_environment() in ["prod", "msit"]:
-            headers = self._get_headers()
-            return self.http.get("https://api.powerbi.com/powerbi/globalservice/v201606/clusterdetails", headers=headers).json()["clusterUrl"] + "/"
-        else:
-            return _get_synapse_endpoint()
+import uuid
+from abc import ABC, abstractmethod
+from dataclasses import dataclass
+import time
+from tqdm.auto import tqdm
+
+from sempy.fabric.exceptions import FabricHTTPException
+from requests.adapters import HTTPAdapter, Retry
+from requests.sessions import Session
+from sempy._utils._log import log_retry, log_rest_response, log_rest_request
+from sempy.fabric._token_provider import SynapseTokenProvider, TokenProvider
+
+from sempy.fabric._environment import _get_synapse_endpoint, _get_environment, _get_fabric_rest_endpoint
+from typing import Dict, Optional
+
+
+class RetryWithLogging(Retry):
+    @log_retry
+    def increment(self, *args, **kwargs):
+        return super().increment(*args, **kwargs)
+
+
+class SessionWithLogging(Session):
+    @log_rest_request
+    def prepare_request(self, *args, **kwargs):
+        return super().prepare_request(*args, **kwargs)
+
+
+@dataclass
+class OperationStatus:
+    status: str
+    retry_after: int
+    percent_complete: int
+
+
+class OperationStart:
+    operation_id: str
+    retry_after: int
+
+    def __init__(self, response):
+        self.operation_id = response.headers.get("x-ms-operation-id") or response.headers['Location'].split('/')[-1]
+        self.retry_after = int(response.headers.get("Retry-After", 2))
+
+
+class BaseRestClient(ABC):
+    """
+    REST client to access Fabric and PowerBI endpoints. Authentication tokens are automatically acquired from the execution environment.
+
+    ***Experimental***: This class is experimental and may change in future versions.
+
+    Parameters
+    ----------
+    token_provider : TokenProvider, default=None
+        Implementation of TokenProvider that can provide auth token
+        for access to the PowerBI workspace. Will attempt to acquire token
+        from its execution environment if not provided.
+    """
+    def __init__(self, token_provider: Optional[TokenProvider] = None):
+        self.http = SessionWithLogging()
+
+        @log_rest_response
+        def validate_rest_response(response, *args, **kwargs):
+            if response.status_code >= 400:
+                raise FabricHTTPException(response)
+        self.http.hooks["response"] = [validate_rest_response]
+        retry_strategy = RetryWithLogging(
+            total=10,
+            allowed_methods=["HEAD", "GET", "POST", "PUT", "PATCH", "DELETE"],
+            status_forcelist=[429, 500, 502, 503, 504],
+            backoff_factor=1
+        )
+        retry_adapter = HTTPAdapter(max_retries=retry_strategy)
+        self.http.mount("https://", retry_adapter)
+
+        self.token_provider = token_provider or SynapseTokenProvider()
+        self.default_base_url = self._get_default_base_url()
+
+    @abstractmethod
+    def _get_default_base_url(self):
+        pass
+
+    def _get_headers(self) -> dict:
+        # this could be static / a function
+        correlation_id = str(uuid.uuid4())
+        return {
+            'authorization': f'Bearer {self.token_provider()}',
+            'Accept': 'application/json',
+            'ActivityId': correlation_id
+        }
+
+    def request(self, method: str, path_or_url: str, *args, **kwargs):
+        """
+        Request to the Fabric and PowerBI REST API.
+
+        Parameters
+        ----------
+        method : str
+            HTTP method.
+        path_or_url : str
+            The path or the url to the resource.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        requests.Response
+            The response from the REST API.
+        """
+        headers = self._get_headers()
+        headers.update(kwargs.get("headers", {}))
+
+        # overwrite url + headers
+        if path_or_url.startswith("https://"):
+            url = path_or_url
+        else:
+            url = f"{self.default_base_url}{path_or_url}"
+
+        kwargs["url"] = url
+        kwargs["headers"] = headers
+
+        return self.http.request(method, *args, **kwargs)
+
+    def get(self, path_or_url: str, *args, **kwargs):
+        """
+        GET request to the Fabric and PowerBI REST API.
+
+        Parameters
+        ----------
+        path_or_url : str
+            The relative path to the resource or the full url.
+            If it's relative, the base URL is automatically prepended.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        requests.Response
+            The response from the REST API.
+        """
+        return self.request("GET", path_or_url, *args, **kwargs)
+
+    def post(self, path_or_url: str, *args, **kwargs):
+        """
+        POST request to the Fabric and PowerBI REST API.
+
+        Parameters
+        ----------
+        path_or_url : str
+            The relative path to the resource or the full url.
+            If it's relative, the base URL is automatically prepended.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        requests.Response
+            The response from the REST API.
+        """
+        return self.request("POST", path_or_url, *args, **kwargs)
+
+    def delete(self, path_or_url: str, *args, **kwargs):
+        """
+        DELETE request to the Fabric and PowerBI REST API.
+
+        Parameters
+        ----------
+        path_or_url : str
+            The relative path to the resource or the full url.
+            If it's relative, the base URL is automatically prepended.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        requests.Response
+            The response from the REST API.
+        """
+        return self.request("DELETE", path_or_url, *args, **kwargs)
+
+    def head(self, path_or_url: str, *args, **kwargs):
+        """
+        HEAD request to the Fabric and PowerBI REST API.
+
+        Parameters
+        ----------
+        path_or_url : str
+            The relative path to the resource or the full url.
+            If it's relative, the base URL is automatically prepended.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        requests.Response
+            The response from the REST API.
+        """
+        return self.request("HEAD", path_or_url, *args, **kwargs)
+
+    def patch(self, path_or_url: str, *args, **kwargs):
+        """
+        PATCH request to the Fabric and PowerBI REST API.
+
+        Parameters
+        ----------
+        path_or_url : str
+            The relative path to the resource or the full url.
+            If it's relative, the base URL is automatically prepended.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        requests.Response
+            The response from the REST API.
+        """
+        return self.request("PATCH", path_or_url, *args, **kwargs)
+
+    def put(self, path_or_url: str, *args, **kwargs):
+        """
+        PUT request to the Fabric and PowerBI REST API.
+
+        Parameters
+        ----------
+        path_or_url : str
+            The relative path to the resource or the full url.
+            If it's relative, the base URL is automatically prepended.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        requests.Response
+            The response from the REST API.
+        """
+        return self.request("PUT", path_or_url, *args, **kwargs)
+
+
+class FabricRestClient(BaseRestClient):
+    """
+    REST client to access Fabric REST endpoints. Authentication tokens are automatically acquired from the execution environment.
+
+    All methods (get, post, ...) have an additional parameter `lro_wait` that can be set to True to wait for the long-running-operation to complete.
+    ***Experimental***: This class is experimental and may change in future versions.
+
+    Parameters
+    ----------
+    token_provider : TokenProvider, default=None
+        Implementation of TokenProvider that can provide auth token
+        for access to the PowerBI workspace. Will attempt to acquire token
+        from its execution environment if not provided.
+    """
+    def __init__(self, token_provider: Optional[TokenProvider] = None):
+        super().__init__(token_provider)
+
+    def _get_default_base_url(self):
+        return _get_fabric_rest_endpoint()
+
+    def request(self,
+                method: str,
+                path_or_url: str,
+                lro_wait: Optional[bool] = False,
+                lro_max_attempts: int = 10,
+                lro_operation_name: Optional[str] = None,
+                *args,
+                **kwargs):
+        """
+        Request to the Fabric REST API.
+
+        Parameters
+        ----------
+        method : str
+            HTTP method.
+        path_or_url : str
+            The path or the url to the resource.
+        lro_wait : bool, default=False
+            If True, waits for the long-running-operation to complete.
+        lro_max_attempts : int, default=10
+            The maximum number of attempts to wait for the long-running-operation to complete.
+        lro_operation_name : str, default=None
+            The name of the operation to wait for displayed via TQDM.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        requests.Response
+            The response from the REST API.
+        """
+        response = super().request(method, path_or_url, *args, **kwargs)
+
+        if not lro_wait or response.status_code != 202:
+            return response
+
+        return self._wait_for_operation(lro_operation_name or path_or_url,
+                                        OperationStart(response),
+                                        lro_max_attempts)
+
+    def get_paged(self, path_or_url: str, headers: Optional[Dict] = None, *args, **kwargs) -> list:
+        """
+        GET request to the Fabric REST API that handles pagination.
+
+        Parameters
+        ----------
+        path_or_url : str
+            The relative path to the resource or the full url.
+            If it's relative, the base URL is automatically prepended.
+        headers : dict, default=None
+            Headers to be included in the request.
+        *args : list
+            Arguments passed to the request method.
+        **kwargs : dict
+            Arguments passed to the request method.
+
+        Returns
+        -------
+        list
+            The list of rows from the response.
+        """
+
+        if headers is None:
+            headers = {}
+
+        headers['Content-Type'] = 'application/json'
+        response = self.get(path_or_url, *args, headers=headers, **kwargs)
+
+        if response.status_code != 200:
+            raise FabricHTTPException(response)
+
+        rows = []
+        while True:
+            response_json = response.json()
+            rows.extend(response_json['value'])
+
+            continuation_uri = response_json.get("continuationUri")
+            if continuation_uri is None:
+                break
+
+            response = self.get(continuation_uri, headers=headers)
+            if response.status_code != 200:
+                raise FabricHTTPException(response)
+
+        return rows
+
+    def _get_operation_status(self, operation_id: str) -> OperationStatus:
+        response = self.get(
+            f"v1/operations/{operation_id}",
+            headers={'Content-Type': 'application/json'}
+        )
+        response_json = response.json()
+
+        return OperationStatus(
+            response_json['status'],
+            int(response.headers.get("Retry-After", 2)),
+            response_json["percentComplete"])
+
+    def _wait_for_operation(self, name, operation: OperationStart, max_attempts: int = 10) -> str:
+        bar = tqdm(total=100, desc=f"Waiting {operation.retry_after} seconds for {name} operation to check for status")
+
+        operation_id = operation.operation_id
+
+        for _ in range(max_attempts):
+            time.sleep(operation.retry_after)
+
+            op_status = self._get_operation_status(operation_id)
+            if op_status.status in ['Failed', 'Undefined']:
+                raise RuntimeError(f"Operation {name} {operation_id} failed: {op_status.status}")
+
+            if op_status.status == 'Succeeded':
+                bar.set_description(f"Operation {name} successfully completed")
+                bar.update(100)
+
+                return self.get(
+                    f"v1/operations/{operation_id}/result",
+                    headers={'Content-Type': 'application/json'}
+                )
+
+            bar.set_description(f"Waiting for {name} to complete: {op_status.status}")
+            bar.update(op_status.percent_complete)
+
+        raise TimeoutError("Operation timed out.")
+
+
+class PowerBIRestClient(BaseRestClient):
+    """
+    REST client to access PowerBI REST endpoints. Authentication tokens are automatically acquired from the execution environment.
+
+    ***Experimental***: This class is experimental and may change in future versions.
+
+    Parameters
+    ----------
+    token_provider : TokenProvider, default=None
+        Implementation of TokenProvider that can provide auth token
+        for access to the PowerBI workspace. Will attempt to acquire token
+        from its execution environment if not provided.
+    """
+    def __init__(self, token_provider: Optional[TokenProvider] = None):
+        super().__init__(token_provider)
+
+    def _get_default_base_url(self):
+        # The endpoint api.powerbi.com does not work for REST calls using the "pbi" token due to limited audience
+        if _get_environment() in ["prod", "msit"]:
+            headers = self._get_headers()
+            return self.http.get("https://api.powerbi.com/powerbi/globalservice/v201606/clusterdetails", headers=headers).json()["clusterUrl"] + "/"
+        else:
+            return _get_synapse_endpoint()
```

## sempy/fabric/_client/_workspace_client.py

```diff
@@ -472,58 +472,41 @@
                                 ("id",                 "Id",                   "str"),
                                 ("displayName",        "Display Name",         "str"),
                                 ("description",        "Description",          "str"),
                                 ("type",               "Type",                 "str"),
                                 ("workspaceId",        "Workspace Id",         "str")])
 
     def create_lakehouse(self, display_name: str, description: Optional[str] = None, max_attempts: int = 10) -> str:
-        operation_or_id = self._fabric_rest_api.create_lakehouse(self.get_workspace_id(), display_name, description)
-
-        return self._wait_for_operation("create lakehouse", operation_or_id, max_attempts)
+        return self._fabric_rest_api.create_lakehouse(self.get_workspace_id(), display_name, description, lro_max_attempts=max_attempts)
 
     def create_workspace(self, display_name: str, description: Optional[str] = None) -> str:
         return self._fabric_rest_api.create_workspace(display_name, description)
 
     def delete_item(self, item_id: str):
         self._fabric_rest_api.delete_item(self.get_workspace_id(), item_id)
 
     def delete_workspace(self):
         self._fabric_rest_api.delete_workspace(self.get_workspace_id())
 
     def create_notebook(self, display_name: str, description: Optional[str] = None, content: Optional[str] = None, max_attempts: int = 10) -> str:
-        operation_or_id = self._fabric_rest_api.create_notebook(self.get_workspace_id(), display_name, description, content)
-
-        return self._wait_for_operation("create notebook", operation_or_id, max_attempts)
-
-    def _wait_for_operation(self, name, operation_or_id: Union[str, OperationStart], max_attempts: int = 10) -> str:
-        if isinstance(operation_or_id, str):
-            return operation_or_id
-
-        bar = tqdm(total=100, desc=f"Waiting {operation_or_id.retry_after} seconds for {name} operation to check for status")
-
-        operation_id = operation_or_id.operation_id
-        time.sleep(operation_or_id.retry_after)
-
-        for _ in range(max_attempts):
-            op_status = self._fabric_rest_api.get_operation_status(operation_id)
-            if op_status.status in ['Failed', 'Undefined']:
-                raise RuntimeError(f"Operation {name} {operation_id} failed: {op_status.status}")
-
-            if op_status.status == 'Succeeded':
-                bar.set_description(f"Operation {name} successfully completed")
-                bar.update(100)
-
-                return self._fabric_rest_api.get_operation_result(operation_id)
-
-            bar.set_description(f"Waiting for {name} to complete: {op_status.status}")
-            bar.update(op_status.percent_complete)
-
-            time.sleep(op_status.retry_after)
+        return self._fabric_rest_api.create_notebook(self.get_workspace_id(), display_name, description, content, max_attempts)
 
-        raise TimeoutError("Operation timed out.")
+    def resolve_item_id(self, item_name: str, type: Optional[str] = None) -> str:
+        df = self.list_items(type=type)
+        selected_df = df[df["Display Name"] == item_name]["Id"]
+        if selected_df.empty:
+            raise ValueError(f"There's no item with the name '{item_name}' in workspace '{self.get_workspace_name()}'")
+        return selected_df.values[0]
+
+    def resolve_item_name(self, item_id: UUID, type: Optional[str] = None) -> str:
+        df = self.list_items(type=type)
+        selected_df = df[df["Id"] == str(item_id)]["Display Name"]
+        if selected_df.empty:
+            raise ValueError(f"There's no item with the ID '{item_id}' in workspace '{self.get_workspace_name()}'")
+        return selected_df.values[0]
 
     def run_notebook_job(self, notebook_id: str, max_attempts: int = 10) -> str:
         workspace_id = self.get_workspace_id()
         op_start = self._fabric_rest_api.run_notebook_job(workspace_id, notebook_id)
 
         self._wait_for_job("notebook", workspace_id, notebook_id, op_start, max_attempts)
```

## sempy/fabric/_dataframe/_fabric_dataframe.py

```diff
@@ -1,14 +1,14 @@
 import pandas as pd
 from uuid import UUID
 from numpy import ndarray
 from typing import Any, Callable, List, Iterable, Dict, Union, Optional, Tuple, TYPE_CHECKING
 from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, BooleanType, BinaryType, TimestampType
 
-from sempy.fabric.exceptions import WorkspaceNotFoundException
+from sempy.fabric.exceptions import WorkspaceNotFoundException, DatasetNotFoundException
 from sempy.fabric._utils import SparkConfigTemporarily
 from sempy.dependencies._find import _find_dependencies_with_stats
 from sempy.dependencies._stats import DataFrameDependencyStats
 from sempy.functions import _SDataFrame
 from sempy.dependencies._validate import (
     _drop_dependency_violations,
     _list_dependency_violations,
@@ -77,15 +77,15 @@
             from sempy.fabric._cache import _get_or_create_workspace_client
             from Microsoft.AnalysisServices import OperationException
 
             try:
                 self.column_metadata = _get_or_create_workspace_client(workspace) \
                     .get_dataset_client(dataset) \
                     .resolve_metadata(self.columns, verbose)
-            except (WorkspaceNotFoundException, OperationException) as e:
+            except (WorkspaceNotFoundException, OperationException, DatasetNotFoundException) as e:
                 if verbose > 0:
                     print(f"Warning: failed to resolve column metadata: {e}")
 
                 self.column_metadata = {}
 
     @property
     def _constructor_sliced(self) -> Callable:
@@ -160,15 +160,15 @@
             'bytes': BinaryType(),
             'datetime64[ns]': TimestampType(),
             'bool': BooleanType()
         }
 
         schema = StructType(
             [
-                StructField(name, conversion_map.get(str(dtype), StringType()))
+                StructField(name, conversion_map.get(str(dtype), StringType()))  # type: ignore
                 for name, dtype in self.dtypes.items()
             ]
         )
 
         return schema
 
     @log
@@ -243,15 +243,15 @@
             left_on=left_on,
             right_on=right_on)
 
         # remove duplicate join columns
         duplicate_join_columns = [c for c in right_on if c not in left_on]
         df_output.drop(columns=duplicate_join_columns, inplace=True)
 
-        return df_output
+        return df_output  # type: ignore
 
     @log
     def find_dependencies(
         self,
         dropna: bool = False,
         threshold: float = 0.01,
         verbose: int = 0
@@ -479,15 +479,15 @@
 
         Returns
         -------
         FabricDataFrame
             New dataframe with constraint determinant -> dependent enforced.
         """
 
-        return _drop_dependency_violations(self, determinant_col, dependent_col, verbose=verbose)
+        return _drop_dependency_violations(self, determinant_col, dependent_col, verbose=verbose)  # type: ignore
 
     @log
     def to_lakehouse_table(self,
                            name: str,
                            mode: Optional[str] = "error",
                            spark_schema: Optional[StructType] = None,
                            delta_column_mapping_mode: str = "name") -> None:
@@ -524,16 +524,16 @@
                 conversion_map[col_name] = 'object'
             elif col_type == 'datetime64[ns]':
                 # Spark wants python datetime. To convert pandas doc advise to use to_pydatetime,
                 # which seems to leave pd.NaT (Not-a-Time) alone. So far, no better conversion
                 # method found, even by Mr. Bing/ChatGPT:
                 df_converted[col_name] = df_converted[col_name].apply(lambda v: v.to_pydatetime())
 
-        df_converted = df_converted.astype(conversion_map)
-        df_converted = df_converted.replace(pd.NA, None)
+        df_converted = df_converted.astype(conversion_map)  # type: ignore
+        df_converted = df_converted.replace(pd.NA, None)  # type: ignore
 
         # Adomd uses "1899-12-30" as a "zero date" following Excel/Access/SQL-Server going
         # back to Lotus 1-2-3 compatibility. We have little choice but to follow:
         #
         # https://stackoverflow.com/questions/3963617/why-is-1899-12-30-the-zero-date-in-access-sql-server-instead-of-12-31
         #
         #  Trying to write such a date fails on a default Spark installation with an exception:
```

## sempy/fabric/_trace/_trace_connection.py

```diff
@@ -125,15 +125,15 @@
         """
         event_df = self._dataset_client._evaluate_dax("SELECT * FROM $System.DISCOVER_TRACE_EVENT_CATEGORIES")
         columns = self._parse_column_schema()
 
         rows = []
         for i in range(event_df.shape[0]):
             xml_data = event_df.iloc[i, 0]
-            event_category_elem = ET.fromstring(xml_data)
+            event_category_elem = ET.fromstring(xml_data)  # type: ignore
             category_rows = self._parse_event_category(event_category_elem, columns)
             rows.extend(category_rows)
 
         cols = ["Event Category Name", "Event Category Description", "Event Name", "Event ID", "Event Description", "Event Column Name", "Event Column ID", "Event Column Description"]
         df = pd.DataFrame(rows, columns=cols)
         return df
```

## sempy/functions/_function.py

```diff
@@ -11,14 +11,16 @@
     _default_auto_args_series,
     _default_requirement_func_series
 )
 
 if TYPE_CHECKING:
     from sempy.fabric._dataframe._fabric_dataframe import FabricDataFrame
     from sempy.fabric._dataframe._fabric_series import FabricSeries
+    from sempy._metadata._mdataframe import MDataFrame
+    from sempy._metadata._mseries import MSeries
 
 
 class SemanticFunction:
     """
     A base class for functions that can be suggested to the user based on semantic context.
 
     Parameters
@@ -101,37 +103,37 @@
         any
             Value returned from this SemanticFunction's _function_.
         """
         if not self.is_applicable(df_or_series):
             raise TypeError(f"Can't apply {self} to {df_or_series}")
         return self.wrapped_function(df_or_series, *args, **kwargs)
 
-    def is_applicable(self, df_or_series: Union["FabricDataFrame", "FabricSeries"]) -> bool:
+    def is_applicable(self, df_or_series: Union["MDataFrame", "MSeries"]) -> bool:
         """
         Return True if function can be applied to the provided FabricDataFrame.
 
         Parameters
         ----------
-        df_or_series : Union[FabricDataFrame, FabricSeries]
+        df_or_series : Union[MDataFrame, MSeries]
              Target semantic data frame.
 
         Returns
         -------
         bool
             Return True if function can be applied to the provided FabricDataFrame.
         """
         return self.requirement(df_or_series)
 
-    def suggest_signature(self, df_or_series: Union["FabricDataFrame", "FabricSeries"]) -> List[str]:
+    def suggest_signature(self, df_or_series: Union["MDataFrame", "MSeries"]) -> List[str]:
         """
         Suggest signature for the provided FabricDataFrame.
 
         Parameters
         ----------
-        df_or_series : Union[FabricDataFrame, FabricSeries]
+        df_or_series : Union[MDataFrame, MSeries]
             Target semantic data frame.
 
         Returns
         -------
         list of str
             List of suggestions.
         """
```

## sempy/functions/_matcher_dataframe.py

```diff
@@ -7,15 +7,15 @@
 from sempy.functions._util import _ParameterRequirement, _extract_requirement
 
 
 def _find_first_match(
     matcher: SeriesMatcher, columns: List[str], df: MDataFrame
 ) -> Optional[str]:
     for col in columns:
-        if matcher.matches(df[col]):
+        if matcher.matches(df[col]):  # type: ignore
             return col
 
     return None
 
 
 def _get_or_create_parameter_requirements(func) -> List[_ParameterRequirement]:
     if not hasattr(func, "__parameter_requirements_dataframe__"):
@@ -33,15 +33,15 @@
         columns = list(df.columns)
 
         for req in requirements:
             # skip anonymous parameters (e.g. NameMatcher() is passed as positional arg to semantic_parameters)
             if req.name is None:
                 continue
 
-            col_matches = [col for col in columns if req.matcher.matches(df[col])]
+            col_matches = [col for col in columns if req.matcher.matches(df[col])]  # type: ignore
 
             col_matches_len = len(col_matches)
 
             # mandatory columns must match
             # we must get a unique match
             if col_matches_len == 1:
                 param_assignments[req.name] = col_matches[0]
@@ -99,15 +99,15 @@
                 return [", ".join(map(lambda x: x[1], args))]
 
             # use dict instead of set to maintain order
             all: Dict[str, None] = dict()
             for idx, req in requirements:
                 result = []
                 for col in columns:
-                    if req.matcher.matches(df[col]):
+                    if req.matcher.matches(df[col]):  # type: ignore
                         if req.is_list:
                             arg = f"{req.name}=['{col}']"
                         else:
                             arg = f"{req.name}='{col}'"
 
                         new_args = list(args)
                         new_args.append((idx, arg))
```

## sempy/functions/matcher/_matcher.py

```diff
@@ -96,15 +96,15 @@
             True if the matcher is met.
         """
 
         # need to be a bit careful w/ object as it matches with everything
         if series.dtype == np.dtype('object'):
             return self.dtype == Any or self.dtype == any or self.dtype == str
 
-        return self._normalize(series.dtype) == self.dtype
+        return self._normalize(series.dtype) == self.dtype  # type: ignore
 
 
 class NameMatcher(SeriesMatcher):
     """
     A column matcher that checks the name of the column matches.
 
     Parameters
```

## sempy/relationships/_find.py

```diff
@@ -12,15 +12,15 @@
 
 
 @log_tables
 def find_relationships(
     tables: Union[Dict[str, pd.DataFrame], List[pd.DataFrame]],
     coverage_threshold: float = 1.0,
     name_similarity_threshold: float = 0.8,
-    exclude: Union[List[Tuple[str]], pd.DataFrame] = None,
+    exclude: Optional[Union[List[Tuple[str]], pd.DataFrame]] = None,
     include_many_to_many: bool = False,
     verbose: int = 0
 ) -> pd.DataFrame:
     """
     Suggest possible relationships based on coverage threshold.
 
     By default `include_many_to_many` is `False`, which is the most common case.
```

## sempy/relationships/_stats.py

```diff
@@ -65,18 +65,18 @@
         # - merge results are reproducible from run to run, which was not true of sets
         self.value_counts = df[[column]]\
                                 .rename(columns={column: VALUE})\
                                 .groupby(VALUE, as_index=False, sort=False, dropna=True)\
                                 .size()
 
         self.nrows = len(df)
-        self.null_count = self.nrows - self.value_counts[SIZE].sum()
+        self.null_count = self.nrows - self.value_counts[SIZE].sum()  # type: ignore
         self.nunique = len(self.value_counts)
         if (self.nunique > 0):
-            self.max_value_count = self.value_counts[SIZE].max()
+            self.max_value_count = self.value_counts[SIZE].max()  # type: ignore
         else:
             self.max_value_count = 0
 
     def intersect_count(self, other):
         """
         Computes the size of an intersect of unique values
```

## sempy/relationships/_utils.py

```diff
@@ -37,29 +37,29 @@
             if value is obj:
                 return name
         frame = frame.f_back
     return None
 
 
 def _to_exclude_tuples(
-    exclude: Union[List[Tuple[str]], pd.DataFrame]
+    exclude: Optional[Union[List[Tuple[str]], pd.DataFrame]]
 ) -> Optional[Set[Tuple[str]]]:
     msg = "not a list of tuples(from_table, from_column, to_table, to_column)"
     if exclude is None:
         result = None
     elif isinstance(exclude, list):
         for t in exclude:
             if not isinstance(t, tuple):
                 raise TypeError(f"Invalid type {type(t)} of an element of \"exclude\": {msg}")
             if len(t) != 4:
                 raise ValueError(f"Invalid len {len(t)} of an element of \"exclude\": {msg}")
         result = set(exclude)
     elif isinstance(exclude, pd.DataFrame):
-        result = exclude[["From Table", "From Column", "To Table", "To Column"]]
-        result = set(result.itertuples(index=False, name=None))   # type: ignore
+        df_sub = exclude[["From Table", "From Column", "To Table", "To Column"]]
+        result = set(df_sub.itertuples(index=False, name=None))   # type: ignore
     else:
         raise TypeError(f"Unexpected type {type(exclude)} for \"exclude\": {msg}")
     return result
 
 
 def _is_key_missing(rel, table_columns, action):
```

## Comparing `semantic_link_sempy-0.7.4.dist-info/LICENSE.txt` & `semantic_link_sempy-0.7.5.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `semantic_link_sempy-0.7.4.dist-info/RECORD` & `semantic_link_sempy-0.7.5.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,79 +1,79 @@
 sempy/__init__.py,sha256=dWTQUV0PwmqLhH16VILNwlVOe4rS9ualRDHImePARXY,1097
-sempy/_version.py,sha256=bx7kBkYafZVe35csFUbAJBOgFJmUn0IMZ03ksgQMv10,497
+sempy/_version.py,sha256=5vSSSZWR32GQTcbdt-Z82E1xtJSfrUGXc_pr53efvIs,497
 sempy/dotnet.runtime.config.json,sha256=syhDFQv6cEmZnE1WtFjNe3NwhsIsnd-CFULv-vEWOFI,167
 sempy/_metadata/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sempy/_metadata/_mdataframe.py,sha256=g2__5z9lbykHxEqfnUPrzK6ONmYXtRt-L3NSM5zlq5s,11141
+sempy/_metadata/_mdataframe.py,sha256=LQf3lUb7H91Z24Q8vb3hrruZy-ucJDwwR2KDinnC-as,11114
 sempy/_metadata/_meta_utils.py,sha256=vyibpCuB-yhmW0pvjf13LoTs9F7g2AoxbmAyNmOsHmQ,828
-sempy/_metadata/_mseries.py,sha256=m_QHccOylCAgwEGdaDVTun-6CA7dZ8TYwS3o6vXqC2E,3017
+sempy/_metadata/_mseries.py,sha256=N4g-ly6S5elRuHQLNyBHJAqmafDjb625evQgFLFjgiM,3011
 sempy/_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sempy/_utils/_log.py,sha256=-BCfTW4Xv8uVmM75h_HItEyCZlmxZ6jzJMnqQ_ZBkEw,17194
 sempy/_utils/_ordered_set.py,sha256=y_SiXpSdtviZxgIJjSxnLxGg_2qs-yCn_YX-0EAi8nI,1180
-sempy/_utils/_pandas_utils.py,sha256=3frKYFHqW5tKFjV6Tv2ktMlow3qOAw5PGGWASZh7kCA,6195
+sempy/_utils/_pandas_utils.py,sha256=sCcG105vo_MkEJOiAXuPPIKvEgS8VdTy3uvLgAoWKK0,6205
 sempy/dependencies/__init__.py,sha256=LvB-j3Ul36gHTH43z5tgUgANQHzjP2qeTgh8ytPm2no,109
 sempy/dependencies/_find.py,sha256=nm2z6GYB4l2Dd1rZvQdlebckH1P8jajEZbhvFuXhrZ8,10216
 sempy/dependencies/_plot.py,sha256=-hbsCVhAXg9yepXi6mUfhqBVQG06J_4ZNSFQdcqMCFg,3897
 sempy/dependencies/_stats.py,sha256=820yzmpQuE3d8pPVL0oJsZmgiplba9ta3grsThDX51I,6023
-sempy/dependencies/_validate.py,sha256=e0vhuxldnSPMUhArwdtDjNwItv6KMQxR79Ci4ShOg3A,4181
-sempy/fabric/__init__.py,sha256=8jCjJmp5VUSI44DqtEXtr4LPCv7XxZYZQiN4krGyVjs,3543
+sempy/dependencies/_validate.py,sha256=vjMmGcMazkNbfXm_N27keChdNavXSm1mYR0ZbKhiFZ8,4197
+sempy/fabric/__init__.py,sha256=riq_GgwmGwq1L-pj1jT6xtSK_3dBkoM77mmLok1ih64,3635
 sempy/fabric/_cache.py,sha256=Ta-qFzAYxhUg_YBlQrZUgcYJn2qdNIYP1y3q1VX7fkA,959
 sempy/fabric/_datacategory.py,sha256=3KOnC5swyCcTz2auPOiATeqPc6Kf5HGCkVDvdJ44iR0,495
 sempy/fabric/_daxmagics.py,sha256=w2uhNgHwhJq_DFnHJQLvzW8x8pv20kVGNnSEa7Tpd_E,1520
 sempy/fabric/_environment.py,sha256=BpXC7lUN8U5xFLO0MK2it3mwGlA2qLtfIJFY60aAcus,4946
-sempy/fabric/_flat.py,sha256=g7mm7meVIZ_pW_B4ImcSkxcmMLnDQhayU8pRjJly28Y,56530
+sempy/fabric/_flat.py,sha256=MyBJesPsiWOC6SwcM-nMRZiq3Leu9wCRN6uSs6tQOig,58829
 sempy/fabric/_flat_list_annotations.py,sha256=tn9MDeEQh104mDV7PG_EeLOFTqDS0KvFh7C0dfmSB9Y,3727
 sempy/fabric/_flat_list_apps.py,sha256=p_lj3-zt2Fn8egCwCBcPajbMkH_FfRadtWlg-WF__po,992
 sempy/fabric/_flat_list_calculation_items.py,sha256=zNL2z57NxBkm1qmU749WYTyQmhyE50klaIoTK4LX6nQ,2932
 sempy/fabric/_flat_list_columns.py,sha256=O9D5h48oxpUUSKG1Hv2JBKCby-LhfRBBZK-nWoSigJM,14677
 sempy/fabric/_flat_list_dataflows.py,sha256=5024cfPday5GE-jcdPiEIDGyiVVT1Tv9mSGqw1yO27E,835
 sempy/fabric/_flat_list_datasources.py,sha256=JM-Nw_kFbHSpNSuMkHB4gyFQmCA9e-zMUZijx9vxoYo,3110
 sempy/fabric/_flat_list_gateways.py,sha256=VJm6wr4Cq_UAwQU7gReGuPILqaNfHg9ICSQMbCUGs2M,765
 sempy/fabric/_flat_list_hierarchies.py,sha256=k6IE6JO55i5wzAPz7Jw9qKzUARlmXnTiS4NiwyW2oU0,5072
-sempy/fabric/_flat_list_partitions.py,sha256=diTLZQHbL_2zNWfJiu6OK0Hr0i_dEeNtKrBYIA7vTzQ,7697
+sempy/fabric/_flat_list_partitions.py,sha256=GGxZFn3V2kpqvRgEzRd-TV3Z9J6ciBz_dZRydefRXa8,7697
 sempy/fabric/_flat_list_perspectives.py,sha256=TErOIp9aLc0glX7Q7io9TYEqCpvH06i-Fe6x16K2GXE,2218
 sempy/fabric/_flat_list_relationships.py,sha256=zG7YHgbZn5pYSaj_x0iaHS76uIbiicfT6Vxp5Kj_biA,7317
 sempy/fabric/_metadatakeys.py,sha256=H1_V1UpeBQB8YksjlRZD8rZwBF0HppxKsKffoCkrWmI,1256
 sempy/fabric/_token_provider.py,sha256=ymXge9mPIF88jYlGzpw92IDwnheMrkuwDwAKFMnlxVU,2945
-sempy/fabric/_utils.py,sha256=rrIbE-gmfBYg3l2B1oVnwBvwxbYXuz2New-yd9DsuuE,9111
+sempy/fabric/_utils.py,sha256=tMC6S-KAoVzf_etPLkaKt7cPIi9APihTFXsUfu9L424,9166
 sempy/fabric/_client/__init__.py,sha256=LYN6FfmHhqjKiTAFhtMCBzqsFCXFKw-OC6R4LIwlOdc,384
 sempy/fabric/_client/_adomd_connection.py,sha256=c_AvSBCchcEOGPmRFHEvhR0MNGRZqTrNDEkTbs652lg,2715
-sempy/fabric/_client/_base_dataset_client.py,sha256=ZpZWS_Kvs3zDdqJJzYx24IDLmvtNA-26znqlaQem1cs,28046
+sempy/fabric/_client/_base_dataset_client.py,sha256=7CCCaMN89yVSup8zbwUFCR9vi5fWumPvj1Ix3PscyH0,28045
 sempy/fabric/_client/_connection_mode.py,sha256=fq0Y5dOJWCyfURO2H9DaQl_X1gTh0dhuly3v6krX4m0,639
 sempy/fabric/_client/_dataset_onelake_import.py,sha256=Dbrp8qkp1UYfmtU50OOsnU2JCUqb1HpWjbZ9U7Abpgs,1541
 sempy/fabric/_client/_dataset_rest_client.py,sha256=4mgCZTUTgBbeBhTH1dGbYFLS8j_8DQCI1Vt_kUJeBpQ,9814
 sempy/fabric/_client/_dataset_xmla_client.py,sha256=cTTRdtpimC1Sn28VzogPex0vkakPpqOoT_p9m5VWPgk,13483
-sempy/fabric/_client/_fabric_rest_api.py,sha256=FxiV2yESqm_HI-ovzAZESIkB-Hq9S9iNEsLjFW43rzo,7719
+sempy/fabric/_client/_fabric_rest_api.py,sha256=4Qe9u3O1IcJjHy7mqgNFZdVYtuNpJSIn-33gkYkSxZA,6128
 sempy/fabric/_client/_pbi_rest_api.py,sha256=MkC8lEGmbQplvW9WzNo15xlAxTjFMEEcyzcW5PB4AxY,14867
 sempy/fabric/_client/_refresh_execution_details.py,sha256=MWyFsIfcVz-Rc64zz7T8ohw2bamw9-IBNijDs7IDGds,1673
-sempy/fabric/_client/_rest_client.py,sha256=AjkbO4DPysnnu_9zQFlumcToW5aIwol3P-472vhReSw,9301
+sempy/fabric/_client/_rest_client.py,sha256=1VSEZdnye1h5dAaI_sbDf58cCOYhphES83a_10IS2Jg,14804
 sempy/fabric/_client/_tools.py,sha256=1jT2AXNGlIEtlyjp2Gi9x595Uf6ndHAft90cGoQZ_Fs,3681
 sempy/fabric/_client/_utils.py,sha256=RWDUtsRgMa1WvQSj4PET0DAvLq97k2QXkcpeoKtsngs,2906
-sempy/fabric/_client/_workspace_client.py,sha256=tzF8CUC8GPdSlNq5rhPsf9bM7GB2KWjYnFCBTSqzYrk,25368
+sempy/fabric/_client/_workspace_client.py,sha256=XSOf1hhB5Ropz3N3GZcNpNdT95f1DmIs2Ss1tukZQmk,24772
 sempy/fabric/_dataframe/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sempy/fabric/_dataframe/_fabric_dataframe.py,sha256=evTCTYe5MKl4HHI3VlBeCIbsa7Ku4x-TW87WpvlWzPE,27600
+sempy/fabric/_dataframe/_fabric_dataframe.py,sha256=iD6ttMAMQLwn9uvX0-6d1emuhqs1BTt6giN6XSMbS-M,27732
 sempy/fabric/_dataframe/_fabric_series.py,sha256=yHxNKV0k645wuhsTXddgkjJHBOdNdhE5revxzt0PizY,1764
 sempy/fabric/_trace/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sempy/fabric/_trace/_trace.py,sha256=9L_y969xv7AfZOZKXLNoYn3xQ0Pcqw6n-owKd8iRO6g,11613
-sempy/fabric/_trace/_trace_connection.py,sha256=eKGjT62PgkbjYaQEDOLKN_i-SAi7cDgwQauiBmZaIOs,7110
+sempy/fabric/_trace/_trace_connection.py,sha256=LXmEBCxvta4wFVtkgbdJYQVcpElsruQUCRp9AvHznmI,7126
 sempy/fabric/exceptions/__init__.py,sha256=UOgxb19UduZMsdTulnXpAWyu3iNewQvJYCL_kS4g5VE,288
 sempy/fabric/exceptions/_exceptions.py,sha256=h7msbKn4oGyxHen3L-KnRZMMvfLKgQFQ5UpdW97UO-A,2705
 sempy/fabric/matcher/__init__.py,sha256=dW1vzY_aqFz4LFFJ26BqDCrNm9iZ6YQzpQn3XLrJbP0,503
 sempy/fabric/matcher/_matcher.py,sha256=Xt41iV6KTQTql9G6gxIXP7fdSU6dZnkX4MCZdt9-I2c,3701
 sempy/functions/__init__.py,sha256=KEiAUkIdHncYI6AHaVc2HvGYYS1s7KYH0wXP3VDghPc,462
 sempy/functions/_decorator.py,sha256=R8ent1Ts936vLiogAYRJMm6yjlrUFNc0Nz9gTEI-8lg,6093
-sempy/functions/_function.py,sha256=9jk_VIlJtNd-5bi-Ti2w_u9U6xkDtViUZOWIgyd0gS8,6226
-sempy/functions/_matcher_dataframe.py,sha256=QYiOIh0Lephbwmd5pTVgDirbYFhBJ4CuRQywQtHWCx4,6444
+sempy/functions/_function.py,sha256=sXxVDSxZM8c08LNGGwyGjFHp9N969M3R79oxD4DzelY,6290
+sempy/functions/_matcher_dataframe.py,sha256=wCnYH6IldqjdTyk4LWpcNd1xfYn6hlJkIcIEohApXts,6492
 sempy/functions/_matcher_series.py,sha256=G7zO79JnTEYvg1Osn_yq2Ym4tpZD63Kk2ak_A1Gbi5Q,1602
 sempy/functions/_registry.py,sha256=lPcfwJtV1DZoiSPB2qYseaN0WUi3GUONxhXZjK9MbP4,4810
 sempy/functions/_util.py,sha256=FLl_N1GADwoCfaBSsEm-3YzfmwgchmkHUfFPJeJfztM,2156
 sempy/functions/_dataframe/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sempy/functions/_dataframe/_sdataframe.py,sha256=rs8cIX9kpgYIp0psTkvYTw3v5OdQmntBASEs1u7c3uI,1192
 sempy/functions/_dataframe/_sseries.py,sha256=O34jNlVR5sinDBebU_dQ_oilTd4yDhiGmT4moR4HRYg,1188
 sempy/functions/matcher/__init__.py,sha256=MKuU33A1ECxhY46pJEUoOSFlr9TdteAvKoSCqB1icyc,335
-sempy/functions/matcher/_matcher.py,sha256=peC-hhXhE-P0WLFD9YiH7g0ylGVdR3BYyerffEfSsNU,5880
+sempy/functions/matcher/_matcher.py,sha256=SrfsifT6G63r2TGGePqG_6unuJjx3jgTi1bDS9WWlbA,5896
 sempy/lib/Apache.Arrow.dll,sha256=hT3dfrv6jrL0Hr1m6QLsQyzXZQlEjIO0JdKDwesDZ2Y,170496
 sempy/lib/IronCompress.dll,sha256=k266fECl0PbJzJhaK6FedWeu_j29hAUna065zErgJdY,13312
 sempy/lib/Microsoft.AnalysisServices.AdomdClient.dll,sha256=s-D5KT1JdpakTNcxYxu1OKeGmsVaC6ORbe2gjISwaPw,846392
 sempy/lib/Microsoft.AnalysisServices.Core.dll,sha256=QvK5VbMLvvumiPp14xPiVuKr5vxVBHPR1KpwYKZcd3k,1146416
 sempy/lib/Microsoft.AnalysisServices.Runtime.Core.dll,sha256=-GMhBTIFwyGHgxuI2n3ef9LeXzFxMi_BVmp5Mg3tRTA,98864
 sempy/lib/Microsoft.AnalysisServices.Runtime.Windows.dll,sha256=UsBO2Q234tpNThKtdKWye-hpbBp1pCf7ERyuUvNyFfM,96816
 sempy/lib/Microsoft.AnalysisServices.Tabular.Json.dll,sha256=Vux47roxP1ZzZ18a9mn2hRimPW2FtnTlKDDVRjIywKM,563248
@@ -84,18 +84,18 @@
 sempy/lib/Microsoft.IO.RecyclableMemoryStream.dll,sha256=16gd7FsEaR0aOqXUjWRy6EvWURKRo4f4XjisuQW-hnA,64960
 sempy/lib/Microsoft.Identity.Client.dll,sha256=dFt6IcdI1VuXXjZMvd5hkVYiZNhGGfsY5Bp3zRXY7LA,1402840
 sempy/lib/Microsoft.ML.DataView.dll,sha256=DJyxFgRHKSapziu3x6GUzQ1GhG0ZT6BZVeiHkpaYWyU,48256
 sempy/lib/Parquet.dll,sha256=lh1KdXIMxFPi6QZL9dHxfC9EdccDBEMxpL23imDuFl4,692736
 sempy/lib/Snappier.dll,sha256=J1Ow6eRuoU35HB1Yu6AXK632onDtf6f2kmnTH5spBPU,41472
 sempy/lib/ZstdSharp.dll,sha256=1m_GkXcBMZCydBqxxxJdOwgZzqEgxZ27k-D9s7aRzO8,442368
 sempy/relationships/__init__.py,sha256=1FjMh0veMEwrsChncYjPaCoQwvktnzENJLRzwOW3IwA,383
-sempy/relationships/_find.py,sha256=6xMexd1ahCha3i9ameKbKTxogHf0gcKb1x73NSz6FG0,14768
+sempy/relationships/_find.py,sha256=zsQHR4k-nucLC3c8igOwe0qn546jfxFS8hxLB2dRhKk,14778
 sempy/relationships/_multiplicity.py,sha256=_iSAD2AxBth7upIiUsd0Flf2EwwFQ1-StiLvudn_rJ0,278
 sempy/relationships/_plot.py,sha256=Uu8pDVm9VV3tAV53cJC3ZoPNXU7YNlhB992ZoydvBpg,5548
-sempy/relationships/_stats.py,sha256=OStKIemRVIpCmb9OS85yYovvxGw5V6NfmhGyCi8Nq80,5665
-sempy/relationships/_utils.py,sha256=32KtjQALb7ADuzGNNY2w0u83Pb5DFJXDGWM1DHvS4Us,3209
+sempy/relationships/_stats.py,sha256=aCdVL2-Qy9h0TfMyCyzG2qJ2nQEbSyCKbo1rhepuULM,5697
+sempy/relationships/_utils.py,sha256=oVr02kU70-SMeJq1ipTUDfe0YCgCjJmuaJ2a1Fe-Oe8,3219
 sempy/relationships/_validate.py,sha256=tWOtQgXxg3iX7Exioh_TH4II1bLLNKAIHdT0a3I5zsg,5934
-semantic_link_sempy-0.7.4.dist-info/LICENSE.txt,sha256=s6ujR17r98d7cZDEpzrNo04NvEWOyVr0IPe8Eynozwc,12690
-semantic_link_sempy-0.7.4.dist-info/METADATA,sha256=PKlMU4ovyEyHBoxtwj-PNXuWZYCc8kx6tQGQJUfyWh8,4972
-semantic_link_sempy-0.7.4.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-semantic_link_sempy-0.7.4.dist-info/top_level.txt,sha256=mptJr2o3N2Z496h_roahVjdqNE7Kgsi0mHWtCxiR0ME,6
-semantic_link_sempy-0.7.4.dist-info/RECORD,,
+semantic_link_sempy-0.7.5.dist-info/LICENSE.txt,sha256=s6ujR17r98d7cZDEpzrNo04NvEWOyVr0IPe8Eynozwc,12690
+semantic_link_sempy-0.7.5.dist-info/METADATA,sha256=u8DjOlt_s70szq5H4eAphnsR4iPlaHycM1dOmqteNuY,6862
+semantic_link_sempy-0.7.5.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+semantic_link_sempy-0.7.5.dist-info/top_level.txt,sha256=mptJr2o3N2Z496h_roahVjdqNE7Kgsi0mHWtCxiR0ME,6
+semantic_link_sempy-0.7.5.dist-info/RECORD,,
```

