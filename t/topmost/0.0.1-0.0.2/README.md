# Comparing `tmp/topmost-0.0.1-7-py3-none-any.whl.zip` & `tmp/topmost-0.0.2-5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,29 +1,34 @@
-Zip file size: 59968 bytes, number of entries: 71
+Zip file size: 80242 bytes, number of entries: 86
 -rw-rw-r--  2.0 unx      117 b- defN 23-Sep-08 14:34 topmost/__init__.py
--rw-rw-r--  2.0 unx      223 b- defN 23-Sep-08 14:34 topmost/data/__init__.py
--rw-rw-r--  2.0 unx     2017 b- defN 23-Sep-09 05:15 topmost/data/basic_dataset_handler.py
+-rw-rw-r--  2.0 unx      276 b- defN 24-Mar-07 07:52 topmost/data/__init__.py
+-rw-rw-r--  2.0 unx     3752 b- defN 24-Mar-15 03:25 topmost/data/basic_dataset_handler.py
 -rw-rw-r--  2.0 unx     5009 b- defN 23-Sep-15 15:15 topmost/data/crosslingual_dataset_handler.py
--rw-rw-r--  2.0 unx      624 b- defN 23-Sep-09 09:27 topmost/data/download.py
+-rw-rw-r--  2.0 unx      827 b- defN 24-Mar-14 08:32 topmost/data/download.py
 -rw-rw-r--  2.0 unx     1685 b- defN 23-Sep-08 14:34 topmost/data/download_20ng.py
 -rw-rw-r--  2.0 unx     3608 b- defN 23-Sep-08 14:34 topmost/data/dynamic_dataset_handler.py
--rw-rw-r--  2.0 unx     1223 b- defN 23-Sep-08 14:34 topmost/data/file_utils.py
--rw-rw-r--  2.0 unx      351 b- defN 23-Sep-16 03:52 topmost/evaluations/__init__.py
--rw-rw-r--  2.0 unx     2004 b- defN 23-Sep-14 02:27 topmost/evaluations/classification.py
--rw-rw-r--  2.0 unx     1306 b- defN 23-Sep-09 10:59 topmost/evaluations/clustering.py
+-rw-rw-r--  2.0 unx      952 b- defN 24-Mar-20 09:33 topmost/data/file_utils.py
+-rw-rw-r--  2.0 unx      520 b- defN 24-Mar-07 04:51 topmost/evaluations/__init__.py
+-rw-rw-r--  2.0 unx     1104 b- defN 24-Mar-07 03:04 topmost/evaluations/build_hierarchy.py
+-rw-rw-r--  2.0 unx     1986 b- defN 24-Mar-07 04:22 topmost/evaluations/classification.py
+-rw-rw-r--  2.0 unx     1297 b- defN 24-Mar-07 02:47 topmost/evaluations/clustering.py
+-rw-rw-r--  2.0 unx     9263 b- defN 24-Mar-07 04:48 topmost/evaluations/hierarchy_quality.py
 -rw-rw-r--  2.0 unx     1413 b- defN 23-Sep-16 09:35 topmost/evaluations/topic_coherence.py
--rw-rw-r--  2.0 unx      694 b- defN 23-Sep-16 03:46 topmost/evaluations/topic_diversity.py
--rw-rw-r--  2.0 unx     1257 b- defN 23-Sep-08 14:34 topmost/models/Encoder.py
--rw-rw-r--  2.0 unx      427 b- defN 23-Sep-12 11:43 topmost/models/__init__.py
+-rw-rw-r--  2.0 unx      686 b- defN 24-Mar-07 04:25 topmost/evaluations/topic_diversity.py
+-rw-rw-r--  2.0 unx     1373 b- defN 24-Mar-06 16:18 topmost/models/Encoder copy.py
+-rw-rw-r--  2.0 unx     1326 b- defN 23-Sep-18 13:49 topmost/models/Encoder.py
+-rw-rw-r--  2.0 unx      512 b- defN 24-Mar-06 16:37 topmost/models/__init__.py
+-rw-rw-r--  2.0 unx     3917 b- defN 23-Sep-27 10:44 topmost/models/basic/CombinedTM.py
 -rw-rw-r--  2.0 unx     3758 b- defN 23-Sep-12 12:39 topmost/models/basic/DecTM.py
 -rw-rw-r--  2.0 unx     2512 b- defN 23-Sep-15 08:34 topmost/models/basic/ETM.py
 -rw-rw-r--  2.0 unx     3646 b- defN 23-Sep-12 12:39 topmost/models/basic/ProdLDA.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/basic/__init__.py
+-rw-rw-r--  2.0 unx     4619 b- defN 23-Sep-18 13:51 topmost/models/basic/ECRTM/CombinedECRTM.py
 -rw-rw-r--  2.0 unx     1503 b- defN 23-Sep-12 12:39 topmost/models/basic/ECRTM/ECR.py
--rw-rw-r--  2.0 unx     4506 b- defN 23-Sep-15 08:16 topmost/models/basic/ECRTM/ECRTM.py
+-rw-rw-r--  2.0 unx     4506 b- defN 23-Sep-19 01:56 topmost/models/basic/ECRTM/ECRTM.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/basic/ECRTM/__init__.py
 -rw-rw-r--  2.0 unx     2253 b- defN 23-Sep-15 14:48 topmost/models/basic/NSTM/NSTM.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/basic/NSTM/__init__.py
 -rw-rw-r--  2.0 unx      850 b- defN 23-Sep-08 14:34 topmost/models/basic/NSTM/auto_diff_sinkhorn.py
 -rw-rw-r--  2.0 unx     2941 b- defN 23-Sep-08 14:34 topmost/models/basic/TSCTM/TSC.py
 -rw-rw-r--  2.0 unx     2575 b- defN 23-Sep-14 02:18 topmost/models/basic/TSCTM/TSCTM.py
 -rw-rw-r--  2.0 unx     1675 b- defN 23-Sep-14 02:23 topmost/models/basic/TSCTM/TopicDistQuant.py
@@ -31,43 +36,53 @@
 -rw-rw-r--  2.0 unx     4478 b- defN 23-Sep-12 12:39 topmost/models/crosslingual/NMTM.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/crosslingual/__init__.py
 -rw-rw-r--  2.0 unx     3546 b- defN 23-Sep-12 12:40 topmost/models/crosslingual/InfoCTM/InfoCTM.py
 -rw-rw-r--  2.0 unx     3628 b- defN 23-Sep-12 12:41 topmost/models/crosslingual/InfoCTM/TAMI.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/crosslingual/InfoCTM/__init__.py
 -rw-rw-r--  2.0 unx     9766 b- defN 23-Sep-15 15:26 topmost/models/dynamic/DETM.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/dynamic/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/ProGBN.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-Mar-08 03:21 topmost/models/hierarchical/ProGBN.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/__init__.py
 -rw-rw-r--  2.0 unx     5234 b- defN 23-Sep-15 14:54 topmost/models/hierarchical/HyperMiner/HyperMiner.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/__init__.py
 -rw-rw-r--  2.0 unx      107 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/__init__.py
 -rw-rw-r--  2.0 unx     6632 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/base.py
 -rw-rw-r--  2.0 unx     1386 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/euclidean.py
 -rw-rw-r--  2.0 unx     5117 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/math_util.py
 -rw-rw-r--  2.0 unx     4403 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/poincare.py
+-rw-rw-r--  2.0 unx    10196 b- defN 24-Mar-06 13:51 topmost/models/hierarchical/ProGBN/ProGBN.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-Mar-06 13:44 topmost/models/hierarchical/ProGBN/__init__.py
+-rw-rw-r--  2.0 unx     4957 b- defN 24-Mar-06 13:45 topmost/models/hierarchical/ProGBN/utils.py
 -rw-rw-r--  2.0 unx     7821 b- defN 23-Sep-15 14:53 topmost/models/hierarchical/SawETM/SawETM.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/SawETM/__init__.py
 -rw-rw-r--  2.0 unx     1392 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/SawETM/block.py
+-rw-rw-r--  2.0 unx     1900 b- defN 24-Mar-06 16:39 topmost/models/hierarchical/TraCo/CDDecoder.py
+-rw-rw-r--  2.0 unx     1952 b- defN 24-Mar-08 07:17 topmost/models/hierarchical/TraCo/TPD.py
+-rw-rw-r--  2.0 unx     4262 b- defN 24-Mar-08 07:17 topmost/models/hierarchical/TraCo/TraCo.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-Mar-08 04:46 topmost/models/hierarchical/TraCo/__init__.py
+-rw-rw-r--  2.0 unx      485 b- defN 24-Mar-06 16:16 topmost/models/hierarchical/TraCo/utils.py
 -rw-rw-r--  2.0 unx       41 b- defN 23-Sep-09 07:49 topmost/preprocessing/__init__.py
--rw-rw-r--  2.0 unx    12014 b- defN 23-Sep-10 10:48 topmost/preprocessing/preprocessing.py
--rw-rw-r--  2.0 unx      528 b- defN 23-Sep-08 14:34 topmost/trainers/__init__.py
--rw-rw-r--  2.0 unx      963 b- defN 23-Sep-15 14:40 topmost/trainers/basic/BERTopic_trainer.py
--rw-rw-r--  2.0 unx     2440 b- defN 23-Sep-09 07:21 topmost/trainers/basic/LDA_trainer.py
--rw-rw-r--  2.0 unx     2424 b- defN 23-Sep-09 07:21 topmost/trainers/basic/NMF_trainer.py
+-rw-rw-r--  2.0 unx    12646 b- defN 24-May-23 17:06 topmost/preprocessing/preprocessing.py
+-rw-rw-r--  2.0 unx      580 b- defN 24-Mar-13 10:24 topmost/trainers/__init__.py
+-rw-rw-r--  2.0 unx      920 b- defN 24-Mar-13 10:48 topmost/trainers/basic/BERTopic_trainer.py
+-rw-rw-r--  2.0 unx     2437 b- defN 24-Mar-06 13:07 topmost/trainers/basic/LDA_trainer.py
+-rw-rw-r--  2.0 unx     2421 b- defN 24-Mar-06 13:07 topmost/trainers/basic/NMF_trainer.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/trainers/basic/__init__.py
--rw-rw-r--  2.0 unx     3192 b- defN 23-Sep-09 07:23 topmost/trainers/basic/basic_trainer.py
+-rw-rw-r--  2.0 unx     3214 b- defN 23-Sep-18 13:04 topmost/trainers/basic/basic_contextual_trainer.py
+-rw-rw-r--  2.0 unx     3551 b- defN 24-Mar-07 13:34 topmost/trainers/basic/basic_trainer.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/trainers/crosslingual/__init__.py
--rw-rw-r--  2.0 unx     4038 b- defN 23-Sep-09 07:22 topmost/trainers/crosslingual/crosslingual_trainer.py
--rw-rw-r--  2.0 unx     4169 b- defN 23-Sep-08 14:34 topmost/trainers/dynamic/DTM_trainer.py
+-rw-rw-r--  2.0 unx     4428 b- defN 24-Mar-07 13:53 topmost/trainers/crosslingual/crosslingual_trainer.py
+-rw-rw-r--  2.0 unx     4167 b- defN 24-Mar-06 13:07 topmost/trainers/dynamic/DTM_trainer.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/trainers/dynamic/__init__.py
--rw-rw-r--  2.0 unx     3456 b- defN 23-Sep-09 07:23 topmost/trainers/dynamic/dynamic_trainer.py
--rw-rw-r--  2.0 unx     2375 b- defN 23-Sep-08 14:34 topmost/trainers/hierarchical/HDP_trainer.py
+-rw-rw-r--  2.0 unx     3714 b- defN 24-Mar-07 13:42 topmost/trainers/dynamic/dynamic_trainer.py
+-rw-rw-r--  2.0 unx     2373 b- defN 24-Mar-06 13:07 topmost/trainers/hierarchical/HDP_trainer.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/trainers/hierarchical/__init__.py
--rw-rw-r--  2.0 unx     3967 b- defN 23-Sep-09 07:24 topmost/trainers/hierarchical/hierarchical_trainer.py
+-rw-rw-r--  2.0 unx     4402 b- defN 24-Mar-07 11:57 topmost/trainers/hierarchical/hierarchical_trainer.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/utils/__init__.py
+-rw-rw-r--  2.0 unx      390 b- defN 24-Mar-06 13:05 topmost/utils/basic_utils.py
 -rw-rw-r--  2.0 unx      390 b- defN 23-Sep-08 14:34 topmost/utils/static_utils.py
--rw-rw-r--  2.0 unx    11358 b- defN 23-Sep-18 03:18 topmost-0.0.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx     1081 b- defN 23-Sep-18 03:18 topmost-0.0.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Sep-18 03:18 topmost-0.0.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx        8 b- defN 23-Sep-18 03:18 topmost-0.0.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6625 b- defN 23-Sep-18 03:18 topmost-0.0.1.dist-info/RECORD
-71 files, 160848 bytes uncompressed, 49158 bytes compressed:  69.4%
+-rw-rw-r--  2.0 unx    11358 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    16609 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        8 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     8084 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/RECORD
+86 files, 229548 bytes uncompressed, 67070 bytes compressed:  70.8%
```

## zipnote {}

```diff
@@ -21,44 +21,59 @@
 
 Filename: topmost/data/file_utils.py
 Comment: 
 
 Filename: topmost/evaluations/__init__.py
 Comment: 
 
+Filename: topmost/evaluations/build_hierarchy.py
+Comment: 
+
 Filename: topmost/evaluations/classification.py
 Comment: 
 
 Filename: topmost/evaluations/clustering.py
 Comment: 
 
+Filename: topmost/evaluations/hierarchy_quality.py
+Comment: 
+
 Filename: topmost/evaluations/topic_coherence.py
 Comment: 
 
 Filename: topmost/evaluations/topic_diversity.py
 Comment: 
 
+Filename: topmost/models/Encoder copy.py
+Comment: 
+
 Filename: topmost/models/Encoder.py
 Comment: 
 
 Filename: topmost/models/__init__.py
 Comment: 
 
+Filename: topmost/models/basic/CombinedTM.py
+Comment: 
+
 Filename: topmost/models/basic/DecTM.py
 Comment: 
 
 Filename: topmost/models/basic/ETM.py
 Comment: 
 
 Filename: topmost/models/basic/ProdLDA.py
 Comment: 
 
 Filename: topmost/models/basic/__init__.py
 Comment: 
 
+Filename: topmost/models/basic/ECRTM/CombinedECRTM.py
+Comment: 
+
 Filename: topmost/models/basic/ECRTM/ECR.py
 Comment: 
 
 Filename: topmost/models/basic/ECRTM/ECRTM.py
 Comment: 
 
 Filename: topmost/models/basic/ECRTM/__init__.py
@@ -129,23 +144,47 @@
 
 Filename: topmost/models/hierarchical/HyperMiner/manifolds/math_util.py
 Comment: 
 
 Filename: topmost/models/hierarchical/HyperMiner/manifolds/poincare.py
 Comment: 
 
+Filename: topmost/models/hierarchical/ProGBN/ProGBN.py
+Comment: 
+
+Filename: topmost/models/hierarchical/ProGBN/__init__.py
+Comment: 
+
+Filename: topmost/models/hierarchical/ProGBN/utils.py
+Comment: 
+
 Filename: topmost/models/hierarchical/SawETM/SawETM.py
 Comment: 
 
 Filename: topmost/models/hierarchical/SawETM/__init__.py
 Comment: 
 
 Filename: topmost/models/hierarchical/SawETM/block.py
 Comment: 
 
+Filename: topmost/models/hierarchical/TraCo/CDDecoder.py
+Comment: 
+
+Filename: topmost/models/hierarchical/TraCo/TPD.py
+Comment: 
+
+Filename: topmost/models/hierarchical/TraCo/TraCo.py
+Comment: 
+
+Filename: topmost/models/hierarchical/TraCo/__init__.py
+Comment: 
+
+Filename: topmost/models/hierarchical/TraCo/utils.py
+Comment: 
+
 Filename: topmost/preprocessing/__init__.py
 Comment: 
 
 Filename: topmost/preprocessing/preprocessing.py
 Comment: 
 
 Filename: topmost/trainers/__init__.py
@@ -159,14 +198,17 @@
 
 Filename: topmost/trainers/basic/NMF_trainer.py
 Comment: 
 
 Filename: topmost/trainers/basic/__init__.py
 Comment: 
 
+Filename: topmost/trainers/basic/basic_contextual_trainer.py
+Comment: 
+
 Filename: topmost/trainers/basic/basic_trainer.py
 Comment: 
 
 Filename: topmost/trainers/crosslingual/__init__.py
 Comment: 
 
 Filename: topmost/trainers/crosslingual/crosslingual_trainer.py
@@ -189,26 +231,29 @@
 
 Filename: topmost/trainers/hierarchical/hierarchical_trainer.py
 Comment: 
 
 Filename: topmost/utils/__init__.py
 Comment: 
 
+Filename: topmost/utils/basic_utils.py
+Comment: 
+
 Filename: topmost/utils/static_utils.py
 Comment: 
 
-Filename: topmost-0.0.1.dist-info/LICENSE
+Filename: topmost-0.0.2.dist-info/LICENSE
 Comment: 
 
-Filename: topmost-0.0.1.dist-info/METADATA
+Filename: topmost-0.0.2.dist-info/METADATA
 Comment: 
 
-Filename: topmost-0.0.1.dist-info/WHEEL
+Filename: topmost-0.0.2.dist-info/WHEEL
 Comment: 
 
-Filename: topmost-0.0.1.dist-info/top_level.txt
+Filename: topmost-0.0.2.dist-info/top_level.txt
 Comment: 
 
-Filename: topmost-0.0.1.dist-info/RECORD
+Filename: topmost-0.0.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## topmost/data/__init__.py

```diff
@@ -1,5 +1,6 @@
 from .basic_dataset_handler import BasicDatasetHandler
+from .basic_dataset_handler import RawDatasetHandler
 from .crosslingual_dataset_handler import CrosslingualDatasetHandler
 from .dynamic_dataset_handler import DynamicDatasetHandler
 
 from .download import download_dataset
```

## topmost/data/basic_dataset_handler.py

```diff
@@ -1,36 +1,77 @@
 import torch
 from torch.utils.data import DataLoader
 import numpy as np
 import scipy.sparse
 import scipy.io
+from sentence_transformers import SentenceTransformer
 from . import file_utils
 
 
+def load_contextual_embed(texts, device, model_name="all-mpnet-base-v2", show_progress_bar=True):
+    model = SentenceTransformer(model_name, device=device)
+    embeddings = model.encode(texts, show_progress_bar=show_progress_bar)
+    return embeddings
+
+
+class RawDatasetHandler:
+    def __init__(self, docs, preprocessing, batch_size=200, device='cpu', as_tensor=False, contextual_embed=False):
+
+        rst = preprocessing.preprocess(docs)
+        self.train_data = rst['train_bow']
+        self.train_texts = rst['train_texts']
+        self.vocab = rst['vocab']
+
+        self.vocab_size = len(self.vocab)
+
+        if contextual_embed:
+            self.train_contextual_embed = load_contextual_embed(self.train_texts, device)
+            self.contextual_embed_size = self.train_contextual_embed.shape[1]
+
+        if as_tensor:
+            if contextual_embed:
+                self.train_data = np.concatenate((self.train_data, self.train_contextual_embed), axis=1)
+
+            self.train_data = torch.from_numpy(self.train_data).float().to(device)
+            self.train_dataloader = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)
+
+
 class BasicDatasetHandler:
-    def __init__(self, dataset_dir, batch_size=200, read_labels=False, device='cpu', as_tensor=False):
+    def __init__(self, dataset_dir, batch_size=200, read_labels=False, device='cpu', as_tensor=False, contextual_embed=False):
         # train_bow: NxV
         # test_bow: Nxv
         # word_emeddings: VxD
         # vocab: V, ordered by word id.
 
-        # self.train_bow, self.test_bow, self.train_texts, self.test_texts, self.train_labels, self.test_labels, self.vocab, self.pretrained_WE = 
         self.load_data(dataset_dir, read_labels)
         self.vocab_size = len(self.vocab)
 
         print("===>train_size: ", self.train_bow.shape[0])
         print("===>test_size: ", self.test_bow.shape[0])
         print("===>vocab_size: ", self.vocab_size)
         print("===>average length: {:.3f}".format(self.train_bow.sum(1).sum() / self.train_bow.shape[0]))
 
+        if contextual_embed:
+            self.train_contextual_embed = load_contextual_embed(self.train_texts, device)
+            self.test_contextual_embed = load_contextual_embed(self.test_texts, device)
+            self.contextual_embed_size = self.train_contextual_embed.shape[1]
+
         if as_tensor:
-            self.train_bow = torch.from_numpy(self.train_bow).to(device)
-            self.test_bow = torch.from_numpy(self.test_bow).to(device)
-            self.train_dataloader = DataLoader(self.train_bow, batch_size=batch_size, shuffle=True)
-            self.test_dataloader = DataLoader(self.test_bow, batch_size=batch_size, shuffle=False)
+            if not contextual_embed:
+                self.train_data = self.train_bow
+                self.test_data = self.test_bow
+            else:
+                self.train_data = np.concatenate((self.train_bow, self.train_contextual_embed), axis=1)
+                self.test_data = np.concatenate((self.test_bow, self.test_contextual_embed), axis=1)
+
+            self.train_data = torch.from_numpy(self.train_data).to(device)
+            self.test_data = torch.from_numpy(self.test_data).to(device)
+
+            self.train_dataloader = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)
+            self.test_dataloader = DataLoader(self.test_data, batch_size=batch_size, shuffle=False)
 
     def load_data(self, path, read_labels):
 
         self.train_bow = scipy.sparse.load_npz(f'{path}/train_bow.npz').toarray().astype('float32')
         self.test_bow = scipy.sparse.load_npz(f'{path}/test_bow.npz').toarray().astype('float32')
         self.pretrained_WE = scipy.sparse.load_npz(f'{path}/word_embeddings.npz').toarray().astype('float32')
```

## topmost/data/download.py

```diff
@@ -2,15 +2,20 @@
 import zipfile
 from torchvision.datasets.utils import download_url
 
 
 def download_dataset(dataset_name, cache_path="~/.topmost"):
     cache_path = os.path.expanduser(cache_path)
     raw_filename = f'{dataset_name}.zip'
-    zipped_dataset_url = f"https://raw.githubusercontent.com/BobXWu/TopMost/master/data/{raw_filename}"
+
+    if dataset_name in ['Wikitext-103']:
+        # download from Git LFS.
+        zipped_dataset_url = f"https://media.githubusercontent.com/media/BobXWu/TopMost/main/data/{raw_filename}"
+    else:
+        zipped_dataset_url = f"https://raw.githubusercontent.com/BobXWu/TopMost/master/data/{raw_filename}"
 
     print(zipped_dataset_url)
 
     download_url(zipped_dataset_url, root=cache_path, filename=raw_filename, md5=None)
 
     with zipfile.ZipFile(f'{cache_path}/{raw_filename}', 'r') as zip_ref:
         zip_ref.extractall(cache_path)
```

## topmost/data/file_utils.py

```diff
@@ -1,26 +1,15 @@
 import os
-import argparse
-import yaml
 import json
 
 
 def make_dir(path):
     os.makedirs(path, exist_ok=True)
 
 
-def update_args(args, path):
-    with open(path) as file:
-        config = yaml.safe_load(file)
-        if config:
-            args = vars(args)
-            args.update(config)
-            args = argparse.Namespace(**args)
-
-
 def read_text(path):
     texts = list()
     with open(path, 'r', encoding='utf-8', errors='ignore') as file:
         for line in file:
             texts.append(line.strip())
     return texts
 
@@ -31,15 +20,15 @@
             file.write(text.strip() + '\n')
 
 
 def read_jsonlist(path):
     data = list()
     with open(path, 'r', encoding='utf-8') as input_file:
         for line in input_file:
-            data.append(json.loads(line, encoding='utf-8'))
+            data.append(json.loads(line))
     return data
 
 
 def save_jsonlist(list_of_json_objects, path, sort_keys=True):
     with open(path, 'w', encoding='utf-8') as output_file:
         for obj in list_of_json_objects:
             output_file.write(json.dumps(obj, sort_keys=sort_keys) + '\n')
```

## topmost/evaluations/__init__.py

```diff
@@ -1,8 +1,14 @@
 from .topic_diversity import compute_topic_diversity
-from .topic_diversity import compute_multiaspect_topic_diversity
+from .topic_diversity import multiaspect_topic_diversity
+
 from .clustering import evaluate_clustering
-from .clustering import evaluate_hierarchical_clustering
-from .classification import *
+from .clustering import hierarchical_clustering
+
+from .classification import evaluate_classification
+from .classification import crosslingual_classification
+from .classification import hierarchical_classification
 
 from .topic_coherence import compute_dynamic_TC
 from .topic_coherence import compute_topic_coherence
+
+from .hierarchy_quality import hierarchy_quality
```

## topmost/evaluations/classification.py

```diff
@@ -15,15 +15,15 @@
     results = {
         'acc': accuracy_score(test_labels, preds),
         'macro-F1': f1_score(test_labels, preds, average='macro')
     }
     return results
 
 
-def evaluate_crosslingual_classification(
+def crosslingual_classification(
     train_theta_en,
     train_theta_cn,
     test_theta_en,
     test_theta_cn,
     train_labels_en,
     train_labels_cn,
     test_labels_en,
@@ -41,15 +41,15 @@
         'intra_en': intra_en,
         'intra_cn': intra_cn,
         'cross_en': cross_en,
         'cross_cn': cross_cn
     }
 
 
-def evaluate_hierarchical_classification(train_theta, test_theta, train_labels, test_labels, classifier='SVM', gamma='scale'):
+def hierarchical_classification(train_theta, test_theta, train_labels, test_labels, classifier='SVM', gamma='scale'):
     num_layer = len(train_theta)
     results = defaultdict(list)
 
     for layer in range(num_layer):
         layer_results = evaluate_classification(train_theta[layer], test_theta[layer], train_labels, test_labels, classifier, gamma)
 
         for key in layer_results:
```

## topmost/evaluations/clustering.py

```diff
@@ -30,15 +30,15 @@
 
 
 def evaluate_clustering(theta, labels):
     preds = np.argmax(theta, axis=1)
     return clustering_metric(labels, preds)
 
 
-def evaluate_hierarchical_clustering(test_theta, test_labels):
+def hierarchical_clustering(test_theta, test_labels):
     num_layer = len(test_theta)
     results = defaultdict(list)
 
     for layer in range(num_layer):
         layer_results = evaluate_clustering(test_theta[layer], test_labels)
 
         for key in layer_results:
```

## topmost/evaluations/topic_diversity.py

```diff
@@ -15,14 +15,14 @@
 
 
 def compute_topic_diversity(top_words, _type="TD"):
     TD = compute_TD(top_words)
     return TD
 
 
-def compute_multiaspect_topic_diversity(top_words, _type="TD"):
+def multiaspect_topic_diversity(top_words, _type="TD"):
     TD_list = list()
     for level_top_words in top_words:
         TD = compute_topic_diversity(level_top_words, _type)
         TD_list.append(TD)
 
     return np.mean(TD_list)
```

## topmost/models/Encoder.py

```diff
@@ -17,17 +17,20 @@
 
         self.mean_bn = nn.BatchNorm1d(num_topic, affine=True)
         self.mean_bn.weight.requires_grad = False
         self.logvar_bn = nn.BatchNorm1d(num_topic, affine=True)
         self.logvar_bn.weight.requires_grad = False
 
     def reparameterize(self, mu, logvar):
-        std = torch.exp(0.5 * logvar)
-        eps = torch.randn_like(std)
-        return eps.mul(std).add_(mu)
+        if self.training:
+            std = torch.exp(0.5 * logvar)
+            eps = torch.randn_like(std)
+            return mu + (eps * std)
+        else:
+            return mu
 
     def forward(self, x):
         e1 = F.softplus(self.fc11(x))
         e1 = F.softplus(self.fc12(e1))
         e1 = self.fc1_drop(e1)
         mu = self.mean_bn(self.fc21(e1))
         logvar = self.logvar_bn(self.fc22(e1))
```

## topmost/models/__init__.py

```diff
@@ -1,14 +1,16 @@
 from .basic.ProdLDA import ProdLDA
+from .basic.CombinedTM import CombinedTM
 from .basic.DecTM import DecTM
 from .basic.ETM import ETM
 from .basic.NSTM.NSTM import NSTM
 from .basic.TSCTM.TSCTM import TSCTM
 from .basic.ECRTM.ECRTM import ECRTM
 
 from .crosslingual.NMTM import NMTM
 from .crosslingual.InfoCTM.InfoCTM import InfoCTM
 
 from .dynamic.DETM import DETM
 
 from .hierarchical.SawETM.SawETM import SawETM
 from .hierarchical.HyperMiner.HyperMiner import HyperMiner
+from .hierarchical.TraCo.TraCo import TraCo
```

## topmost/preprocessing/preprocessing.py

```diff
@@ -22,14 +22,16 @@
 replace = re.compile('[%s]' % re.escape(punctuation))
 alpha = re.compile('^[a-zA-Z_]+$')
 alpha_or_num = re.compile('^[a-zA-Z_]+|[0-9_]+$')
 alphanum = re.compile('^[a-zA-Z0-9_]+$')
 
 
 def get_stopwords(stopwords):
+    if stopwords is None:
+        stopwords = []
     if isinstance(stopwords, list):
         stopword_set = stopwords
     elif isinstance(stopwords, str):
         stopword_set = file_utils.read_text(stopwords)
     else:
         raise NotImplementedError(stopwords)
 
@@ -76,15 +78,15 @@
         text = re.sub(r'\'', '', text)
         # replace all whitespace with a single space
         text = re.sub(r'\s', ' ', text)
         # strip off spaces on either end
         text = text.strip()
         return text
 
-    def tokenize(self, text, vocab=None):
+    def tokenize(self, text):
         text = self.clean_text(text, self.strip_html, self.lower)
         tokens = text.split()
 
         tokens = ['_' if t in self.stopword_set else t for t in tokens]
 
         # remove tokens that contain numbers
         if not self.keep_alphanum and not self.keep_num:
@@ -98,44 +100,40 @@
         if self.min_length > 0:
             tokens = [t if len(t) >= self.min_length else '_' for t in tokens]
 
         unigrams = [t for t in tokens if t != '_']
         # counts = Counter()
         # counts.update(unigrams)
 
-        if vocab is not None:
-            tokens = [token for token in unigrams if token in vocab]
-        else:
-            tokens = unigrams
-
-        return tokens
+        return unigrams
 
 
 def make_word_embeddings(vocab):
     glove_vectors = gensim.downloader.load('glove-wiki-gigaword-200')
     word_embeddings = np.zeros((len(vocab), glove_vectors.vectors.shape[1]))
 
     num_found = 0
-    for i, word in enumerate(tqdm(vocab, desc="===>making word embeddings")):
-        try:
-            key_word_list = glove_vectors.index_to_key
-        except:
-            key_word_list = glove_vectors.index2word
 
+    try:
+        key_word_list = glove_vectors.index_to_key
+    except:
+        key_word_list = glove_vectors.index2word
+
+    for i, word in enumerate(tqdm(vocab, desc="===>making word embeddings")):
         if word in key_word_list:
             word_embeddings[i] = glove_vectors[word]
             num_found += 1
 
     print(f'===> number of found embeddings: {num_found}/{len(vocab)}')
 
     return scipy.sparse.csr_matrix(word_embeddings)
 
 
 class Preprocessing:
-    def __init__(self, test_sample_size=None, test_p=0.2, stopwords="snowball", min_doc_count=0, max_doc_freq=1.0, keep_num=False, keep_alphanum=False, strip_html=False, no_lower=False, min_length=3, min_term=1, vocab_size=None, seed=42):
+    def __init__(self, tokenizer=None, test_sample_size=None, test_p=0.2, stopwords=None, min_doc_count=0, max_doc_freq=1.0, keep_num=False, keep_alphanum=False, strip_html=False, no_lower=False, min_length=3, min_term=1, vocab_size=None, seed=42):
         """
         Args:
             test_sample_size:
                 Size of the test set.
             test_p:
                 Proportion of the test set. This helps sample the train set based on the size of the test set.
             stopwords:
@@ -166,103 +164,120 @@
         self.min_doc_count = min_doc_count
         self.max_doc_freq = max_doc_freq
         self.min_term = min_term
         self.test_p = test_p
         self.vocab_size = vocab_size
         self.seed = seed
 
-        self.tokenizer = Tokenizer(stopwords, keep_num, keep_alphanum, strip_html, no_lower, min_length)
+        if tokenizer is not None:
+            self.tokenizer = tokenizer
+        else:
+            self.tokenizer = Tokenizer(stopwords, keep_num, keep_alphanum, strip_html, no_lower, min_length).tokenize
 
     def parse(self, texts, vocab):
         if not isinstance(texts, list):
             texts = [texts]
 
+        vocab_set = set(vocab)
         parsed_texts = list()
         for i, text in enumerate(tqdm(texts, desc="===>parse texts")):
-            tokens = tokens = self.tokenizer.tokenize(text, vocab=vocab)
+            tokens = self.tokenizer(text)
+            tokens = [t for t in tokens if t in vocab_set]
             parsed_texts.append(' '.join(tokens))
 
         vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=lambda x: x.split())
-        bow_matrix = vectorizer.fit_transform(parsed_texts)
-        bow_matrix = bow_matrix.toarray()
-        return parsed_texts, bow_matrix
-
-    def parse_dataset(self, dataset_dir, label_name):
-        np.random.seed(self.seed)
+        bow = vectorizer.fit_transform(parsed_texts)
+        bow = bow.toarray()
+        return parsed_texts, bow
 
+    def preprocess_jsonlist(self, dataset_dir, label_name=None):
         train_items = file_utils.read_jsonlist(os.path.join(dataset_dir, 'train.jsonlist'))
         test_items = file_utils.read_jsonlist(os.path.join(dataset_dir, 'test.jsonlist'))
 
-        n_train = len(train_items)
-        n_test = len(test_items)
+        print(f"Found training documents {len(train_items)} testing documents {len(test_items)}")
 
-        print(f"Found training documents {n_train} testing documents {n_test}")
+        raw_train_texts = []
+        train_labels = []
+        raw_test_texts = []
+        test_labels = []
 
-        all_items = train_items + test_items
-        n_items = len(all_items)
+        for item in train_items:
+            raw_train_texts.append(item['text'])
 
-        if label_name is not None:
-            label_set = set()
-            for i, item in enumerate(all_items):
-                label_set.add(str(item[label_name]))
+            if label_name is not None:
+                train_labels.append(item[label_name])
+ 
+        for item in test_items:
+            raw_test_texts.append(item['text'])
 
-            label_list = list(label_set)
+            if label_name is not None:
+                test_labels.append(item[label_name])
+
+        rst = self.preprocess(raw_train_texts, train_labels, raw_test_texts, test_labels)
+
+        return rst
+
+    def convert_labels(self, train_labels, test_labels):
+        if train_labels is not None:
+            label_list = list(set(train_labels))
             label_list.sort()
             n_labels = len(label_list)
             label2id = dict(zip(label_list, range(n_labels)))
 
-            print("Found label %s with %d classes" % (label_name, n_labels))
             print("label2id: ", label2id)
 
+            train_labels = [label2id[label] for label in train_labels]
+
+            if test_labels is not None:
+                test_labels = [label2id[label] for label in test_labels]
+
+        return train_labels, test_labels
+
+    def preprocess(self, raw_train_texts, train_labels=None, raw_test_texts=None, test_labels=None):
+        np.random.seed(self.seed)
+
         train_texts = list()
         test_texts = list()
-        train_labels = list()
-        test_labels = list()
-
         word_counts = Counter()
         doc_counts_counter = Counter()
 
-        for i, item in enumerate(tqdm(all_items, desc="===>parse texts")):
-            text = item['text']
-            label = label2id[item[label_name]]
+        train_labels, test_labels = self.convert_labels(train_labels, test_labels)
 
-            # tokens = tokenize(text, strip_html=self.strip_html, lower=(not self.no_lower), keep_numbers=self.keep_num, keep_alphanum=self.keep_alphanum, min_length=self.min_length, stopwords=self.stopword_set)
-            tokens = self.tokenizer.tokenize(text)
+        for text in tqdm(raw_train_texts, desc="===>parse train texts"):
+            tokens = self.tokenizer(text)
             word_counts.update(tokens)
             doc_counts_counter.update(set(tokens))
             parsed_text = ' '.join(tokens)
-            # train_texts and test_texts have been parsed.
-            if i < n_train:
-                train_texts.append(parsed_text)
-                train_labels.append(label)
-            else:
+            train_texts.append(parsed_text)
+
+        if raw_test_texts:
+            for text in tqdm(raw_test_texts, desc="===>parse test texts"):
+                tokens = self.tokenizer(text)
+                word_counts.update(tokens)
+                doc_counts_counter.update(set(tokens))
+                parsed_text = ' '.join(tokens)
                 test_texts.append(parsed_text)
-                test_labels.append(label)
 
         words, doc_counts = zip(*doc_counts_counter.most_common())
-        doc_freqs = np.array(doc_counts) / float(n_items)
+        doc_freqs = np.array(doc_counts) / float(len(train_texts) + len(test_texts))
+
         vocab = [word for i, word in enumerate(words) if doc_counts[i] >= self.min_doc_count and doc_freqs[i] <= self.max_doc_freq]
 
         # filter vocabulary
-        if (self.vocab_size is not None) and (len(vocab) > self.vocab_size):
+        if self.vocab_size is not None:
             vocab = vocab[:self.vocab_size]
 
         vocab.sort()
 
-        print(f"Real vocab size: {len(vocab)}")
-
-        print("===>convert to matrix...")
-        vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=lambda x: x.split())
-        bow_matrix = vectorizer.fit_transform(train_texts + test_texts)
-
-        train_bow_matrix = bow_matrix[:len(train_texts)]
-        test_bow_matrix = bow_matrix[-len(test_texts):]
+        train_idx = [i for i, text in enumerate(train_texts) if len(text.split()) >= self.min_term]
+        train_idx = np.asarray(train_idx)
 
-        train_idx = np.where(train_bow_matrix.sum(axis=1) >= self.min_term)[0]
-        test_idx = np.where(test_bow_matrix.sum(axis=1) >= self.min_term)[0]
+        if raw_test_texts is not None:
+            test_idx = [i for i, text in enumerate(test_texts) if len(text.split()) >= self.min_term]
+            test_idx = np.asarray(test_idx)
 
         # randomly sample
         if self.test_sample_size:
             print("===>sample train and test sets...")
 
             train_num = len(train_idx)
             test_num = len(test_idx)
@@ -271,41 +286,54 @@
             if train_sample_size > train_num:
                 test_sample_size = int((train_num / (1 - self.test_p)) * self.test_p)
                 train_sample_size = train_num
 
             train_idx = train_idx[np.sort(np.random.choice(train_num, train_sample_size, replace=False))]
             test_idx = test_idx[np.sort(np.random.choice(test_num, test_sample_size, replace=False))]
 
-            print("===>sampled train size: ", len(train_idx))
-            print("===>sampled test size: ", len(test_idx))
+            print(f"===>sampled train size: {len(train_idx)}")
+            print(f"===>sampled train size: {len(test_idx)}")
 
-        self.train_bow_matrix = train_bow_matrix[train_idx]
-        self.test_bow_matrix = test_bow_matrix[test_idx]
-        self.train_labels = np.asarray(train_labels)[train_idx]
-        self.test_labels = np.asarray(test_labels)[test_idx]
-        self.vocab = vocab
-
-        self.train_texts, _ = self.parse(np.asarray(train_texts)[train_idx].tolist(), vocab)
-        self.test_texts, _ = self.parse(np.asarray(test_texts)[test_idx].tolist(), vocab)
-        self.word_embeddings = make_word_embeddings(vocab)
-
-        print("Real training size: ", len(self.train_texts))
-        print("Real testing size: ", len(self.test_texts))
-        print(f"average length of training set: {self.train_bow_matrix.sum(1).sum() / len(self.train_texts):.3f}")
-        print(f"average length of testing set: {self.test_bow_matrix.sum(1).sum() / len(self.test_texts):.3f}")
+        train_texts, train_bow = self.parse(np.asarray(train_texts)[train_idx].tolist(), vocab)
 
-    def save(self, output_dir):
-        print("Real output_dir is ", output_dir)
-        file_utils.make_dir(output_dir)
+        rst = {
+            'vocab': vocab,
+            'train_bow': train_bow,
+            'train_texts': train_texts,
+            'word_embeddings': make_word_embeddings(vocab)
+        }
 
-        scipy.sparse.save_npz(f"{output_dir}/train_bow.npz", self.train_bow_matrix)
-        scipy.sparse.save_npz(f"{output_dir}/test_bow.npz", self.test_bow_matrix)
+        if train_labels is not None:
+            rst['train_labels'] = np.asarray(train_labels)[train_idx]
+
+        print(f"Real vocab size: {len(vocab)}")
+        print(f"Real training size: {len(train_texts)} \t avg length: {rst['train_bow'].sum() / len(train_texts):.3f}")
+
+        if raw_test_texts is not None:
+            rst['test_texts'], rst['test_bow'] = self.parse(np.asarray(test_texts)[test_idx].tolist(), vocab)
+
+            if test_labels is not None:
+                rst['test_labels'] = np.asarray(test_labels)[test_idx]
+
+            print(f"Real testing size: {len(rst['test_texts'])} \t avg length: {rst['test_bow'].sum() / len(rst['test_texts']):.3f}")
+
+        return rst
+
+    def save(self, output_dir, vocab, train_texts, train_bow, word_embeddings, train_labels=None, test_texts=None, test_bow=None, test_labels=None):
+        file_utils.make_dir(output_dir)
 
-        scipy.sparse.save_npz(f"{output_dir}/word_embeddings.npz", self.word_embeddings)
+        file_utils.save_text(vocab, f"{output_dir}/vocab.txt")
+        file_utils.save_text(train_texts, f"{output_dir}/train_texts.txt")
+        scipy.sparse.save_npz(f"{output_dir}/train_bow.npz", scipy.sparse.csr_matrix(train_bow))
+        scipy.sparse.save_npz(f"{output_dir}/word_embeddings.npz", word_embeddings)
+
+        if train_labels is not None:
+            np.savetxt(f"{output_dir}/train_labels.txt", train_labels, fmt='%i')
 
-        file_utils.save_text(self.train_texts, f"{output_dir}/train_texts.txt")
-        file_utils.save_text(self.test_texts, f"{output_dir}/test_texts.txt")
+        if test_bow is not None:
+            scipy.sparse.save_npz(f"{output_dir}/test_bow.npz", scipy.sparse.csr_matrix(test_bow))
 
-        np.savetxt(f"{output_dir}/train_labels.txt", self.train_labels, fmt='%i')
-        np.savetxt(f"{output_dir}/test_labels.txt", self.test_labels, fmt='%i')
-        file_utils.save_text(self.vocab, f"{output_dir}/vocab.txt")
+        if test_texts is not None:
+            file_utils.save_text(test_texts, f"{output_dir}/test_texts.txt")
 
+            if test_labels is not None:
+                np.savetxt(f"{output_dir}/test_labels.txt", test_labels, fmt='%i')
```

## topmost/trainers/__init__.py

```diff
@@ -1,8 +1,9 @@
 from .basic.basic_trainer import BasicTrainer
+from .basic.BERTopic_trainer import BERTopicTrainer
 from .basic.LDA_trainer import LDAGensimTrainer
 from .basic.LDA_trainer import LDASklearnTrainer
 from .basic.NMF_trainer import NMFGensimTrainer
 from .basic.NMF_trainer import NMFSklearnTrainer
 
 from .crosslingual.crosslingual_trainer import CrosslingualTrainer
 from .dynamic.dynamic_trainer import DynamicTrainer
```

## topmost/trainers/basic/BERTopic_trainer.py

```diff
@@ -1,17 +1,16 @@
 from bertopic import BERTopic
 
 
 class BERTopicTrainer:
-    def __init__(self, dataset, num_topics=50, num_top_words=15):
+    def __init__(self, num_topics=50, num_top_words=15):
         self.model = BERTopic(nr_topics=num_topics, top_n_words=num_top_words)
-        self.dataset = dataset
 
-    def train(self):
-        self.model.fit_transform(self.dataset.train_texts)
+    def train(self, dataset):
+        self.model.fit_transform(dataset.train_texts)
 
     def test(self, texts):
         theta, _ = self.model.approximate_distribution(texts)
         return theta
 
     def export_beta(self):
         # NOTE: beta is modeled as unnormalized c-tf_idf.
@@ -20,11 +19,11 @@
 
     def export_top_words(self):
         top_words = list()
         for item in self.model.get_topics().values():
             top_words.append(' '.join([x[0] for x in item]))
         return top_words
 
-    def export_theta(self):
-        train_theta, _ = self.test(self.dataset.train_texts)
-        test_theta, _ = self.test(self.dataset.test_texts)
+    def export_theta(self, dataset):
+        train_theta = self.test(dataset.train_texts)
+        test_theta = self.test(dataset.test_texts)
         return train_theta, test_theta
```

## topmost/trainers/basic/LDA_trainer.py

```diff
@@ -1,10 +1,10 @@
 import gensim
 from gensim.models import LdaModel
-from topmost.utils import static_utils
+from topmost.utils import basic_utils
 
 
 class LDAGensimTrainer:
     def __init__(self, dataset, num_topics=50, max_iter=1, alpha="symmetric", eta=None):
         self.dataset = dataset
         self.num_topics = num_topics
         self.vocab_size = dataset.vocab_size
@@ -33,15 +33,15 @@
         return theta
 
     def export_beta(self):
         return self.model.get_topics()
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
-        top_words = static_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top=num_top)
+        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top=num_top)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset.train_bow)
         test_theta = self.test(self.dataset.test_bow)
         return train_theta, test_theta
 
@@ -60,14 +60,14 @@
         return self.model.transform(bow.astype('int64'))
 
     def export_beta(self):
         return self.model.components_
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
-        top_words = static_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top=num_top)
+        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top=num_top)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset.train_bow)
         test_theta = self.test(self.dataset.test_bow)
         return train_theta, test_theta
```

## topmost/trainers/basic/NMF_trainer.py

```diff
@@ -1,10 +1,10 @@
 import gensim
 from gensim.models import nmf
-from topmost.utils import static_utils
+from topmost.utils import basic_utils
 
 
 class NMFGensimTrainer:
     def __init__(self, dataset_handler, vocab_size, num_topics=50, max_iter=1):
         self.dataset_handler = dataset_handler
         self.num_topics = num_topics
         self.vocab_size = vocab_size
@@ -30,15 +30,15 @@
         return theta
 
     def export_beta(self):
         return self.model.get_topics()
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
-        top_words = static_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top=num_top)
+        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top=num_top)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset_handler.train_bow)
         test_theta = self.test(self.dataset_handler.test_bow)
         return train_theta, test_theta
 
@@ -57,14 +57,14 @@
         return self.model.transform(bow.astype('int64'))
 
     def export_beta(self):
         return self.model.components_
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
-        top_words = static_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top=num_top)
+        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top=num_top)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset_handler.train_bow)
         test_theta = self.test(self.dataset_handler.test_bow)
         return train_theta, test_theta
```

## topmost/trainers/basic/basic_trainer.py

```diff
@@ -1,19 +1,18 @@
 import numpy as np
+from tqdm import tqdm
 import torch
 from torch.optim.lr_scheduler import StepLR
 from collections import defaultdict
-from tqdm import tqdm
 from topmost.utils import static_utils
 
 
 class BasicTrainer:
-    def __init__(self, model, dataset_handler, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
+    def __init__(self, model, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
         self.model = model
-        self.dataset_handler = dataset_handler
         self.epochs = epochs
         self.learning_rate = learning_rate
         self.batch_size = batch_size
         self.lr_scheduler = lr_scheduler
         self.lr_step_size = lr_step_size
         self.log_interval = log_interval
 
@@ -23,46 +22,56 @@
             'lr': self.learning_rate,
         }
 
         optimizer = torch.optim.Adam(**args_dict)
         return optimizer
 
     def make_lr_scheduler(self, optimizer):
-        lr_scheduler = StepLR(optimizer, step_size=self.lr_step_size, gamma=0.5, verbose=False)
+        if self.lr_scheduler == "StepLR":
+            lr_scheduler = StepLR(optimizer, step_size=self.lr_step_size, gamma=0.5, verbose=False)
+        else:
+            raise NotImplementedError(self.lr_scheduler)
         return lr_scheduler
 
-    def train(self):
+    def fit_transform(self, dataset_handler, num_top_words=15, verbose=False):
+        self.train(dataset_handler, verbose)
+        top_words = self.export_top_words(dataset_handler.vocab, num_top_words)
+        train_theta = self.test(dataset_handler.train_data)
+
+        return top_words, train_theta
+
+    def train(self, dataset_handler, verbose=False):
         optimizer = self.make_optimizer()
 
         if self.lr_scheduler:
-            print("===>Warning: use lr_scheduler")
+            print("===>using lr_scheduler")
             lr_scheduler = self.make_lr_scheduler(optimizer)
 
-        data_size = len(self.dataset_handler.train_dataloader.dataset)
+        data_size = len(dataset_handler.train_dataloader.dataset)
 
         for epoch in tqdm(range(1, self.epochs + 1)):
             self.model.train()
             loss_rst_dict = defaultdict(float)
 
-            for batch_data in self.dataset_handler.train_dataloader:
+            for batch_data in dataset_handler.train_dataloader:
 
                 rst_dict = self.model(batch_data)
                 batch_loss = rst_dict['loss']
 
                 optimizer.zero_grad()
                 batch_loss.backward()
                 optimizer.step()
 
                 for key in rst_dict:
                     loss_rst_dict[key] += rst_dict[key] * len(batch_data)
 
             if self.lr_scheduler:
                 lr_scheduler.step()
 
-            if epoch % self.log_interval == 0:
+            if verbose and epoch % self.log_interval == 0:
                 output_log = f'Epoch: {epoch:03d}'
                 for key in loss_rst_dict:
                     output_log += f' {key}: {loss_rst_dict[key] / data_size :.3f}'
 
                 print(output_log)
 
     def test(self, input_data):
@@ -80,16 +89,16 @@
         theta = np.asarray(theta)
         return theta
 
     def export_beta(self):
         beta = self.model.get_beta().detach().cpu().numpy()
         return beta
 
-    def export_top_words(self, num_top_words=15):
+    def export_top_words(self, vocab, num_top_words=15):
         beta = self.export_beta()
-        top_words = static_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top_words=num_top_words)
+        top_words = static_utils.print_topic_words(beta, vocab, num_top_words)
         return top_words
 
-    def export_theta(self):
-        train_theta = self.test(self.dataset_handler.train_bow)
-        test_theta = self.test(self.dataset_handler.test_bow)
+    def export_theta(self, dataset_handler):
+        train_theta = self.test(dataset_handler.train_data)
+        test_theta = self.test(dataset_handler.test_data)
         return train_theta, test_theta
```

## topmost/trainers/crosslingual/crosslingual_trainer.py

```diff
@@ -1,19 +1,18 @@
-import numpy as np
 import torch
 from torch.optim.lr_scheduler import StepLR
 from collections import defaultdict
+import numpy as np
 from tqdm import tqdm
 from topmost.utils import static_utils
 
 
 class CrosslingualTrainer:
-    def __init__(self, model, dataset_handler, epochs=500, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
+    def __init__(self, model, epochs=500, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
         self.model = model
-        self.dataset_handler = dataset_handler
         self.epochs = epochs
         self.learning_rate = learning_rate
         self.batch_size = batch_size
         self.lr_scheduler = lr_scheduler
         self.lr_step_size = lr_step_size
         self.log_interval = log_interval
 
@@ -24,34 +23,43 @@
         }
 
         optimizer = torch.optim.Adam(**args_dict)
         return optimizer
 
     def make_lr_scheduler(self, optimizer):
         if self.lr_scheduler == 'StepLR':
+            print("===>using lr_scheduler")
             lr_scheduler = StepLR(optimizer, step_size=self.lr_step_size, gamma=0.5, verbose=False)
         else:
             raise NotImplementedError(self.lr_scheduler)
 
         return lr_scheduler
 
-    def train(self):
-        data_size = len(self.dataset_handler.train_dataloader.dataset)
+    def fit_transform(self, dataset_handler, num_top_words=15, verbose=False):
+        self.train(dataset_handler, verbose)
+        top_words_en, top_words_cn = self.export_top_words(dataset_handler.vocab_en, dataset_handler.vocab_cn, num_top_words)
+        train_theta_en, train_theta_cn = self.test(dataset_handler.train_bow_en, dataset_handler.train_bow_cn)
+
+        return top_words_en, top_words_cn, train_theta_en, train_theta_cn
+
+
+    def train(self, dataset_handler, verbose=False):
+        data_size = len(dataset_handler.train_dataloader.dataset)
         optimizer = self.make_optimizer()
 
         if self.lr_scheduler:
             lr_scheduler = self.make_lr_scheduler(optimizer)
 
         for epoch in tqdm(range(1, self.epochs + 1)):
 
 
             loss_rst_dict = defaultdict(float)
 
             self.model.train()
-            for batch_data in self.dataset_handler.train_dataloader:
+            for batch_data in dataset_handler.train_dataloader:
                 batch_bow_en = batch_data['bow_en']
                 batch_bow_cn = batch_data['bow_cn']
                 params_list = [batch_bow_en, batch_bow_cn]
 
                 rst_dict = self.model(*params_list)
 
                 batch_loss = rst_dict['loss']
@@ -63,15 +71,15 @@
                 optimizer.zero_grad()
                 batch_loss.backward()
                 optimizer.step()
 
             if self.lr_scheduler:
                 lr_scheduler.step()
 
-            if epoch % self.log_interval == 0:
+            if verbose and epoch % self.log_interval == 0:
                 output_log = f'Epoch: {epoch:03d}'
                 for key in loss_rst_dict:
                     output_log += f' {key}: {loss_rst_dict[key] / data_size :.3f}'
 
                 print(output_log)
 
     def get_theta(self, bow, lang):
@@ -86,26 +94,29 @@
                 theta_list.extend(theta.detach().cpu().numpy().tolist())
 
         return np.asarray(theta_list)
 
     def test(self, bow_en, bow_cn):
         theta_en = self.get_theta(bow_en, lang='en')
         theta_cn = self.get_theta(bow_cn, lang='cn')
+
         return theta_en, theta_cn
 
     def export_beta(self):
         beta_en, beta_cn = self.model.get_beta()
         beta_en = beta_en.detach().cpu().numpy()
         beta_cn = beta_cn.detach().cpu().numpy()
+
         return beta_en, beta_cn
 
-    def export_top_words(self, num_top_words=15):
+    def export_top_words(self, vocab_en, vocab_cn, num_top_words=15):
         beta_en, beta_cn = self.export_beta()
-        top_words_en = static_utils.print_topic_words(beta_en, vocab=self.dataset_handler.vocab_en, num_top_words=num_top_words)
-        top_words_cn = static_utils.print_topic_words(beta_cn, vocab=self.dataset_handler.vocab_cn, num_top_words=num_top_words)
+        top_words_en = static_utils.print_topic_words(beta_en, vocab_en, num_top_words)
+        top_words_cn = static_utils.print_topic_words(beta_cn, vocab_cn, num_top_words)
 
         return top_words_en, top_words_cn
 
-    def export_theta(self):
-        train_theta_en, train_theta_cn = self.test(self.dataset_handler.train_bow_en, self.dataset_handler.train_bow_cn)
-        test_theta_en, test_theta_cn = self.test(self.dataset_handler.test_bow_en, self.dataset_handler.test_bow_cn)
+    def export_theta(self, dataset_handler):
+        train_theta_en, train_theta_cn = self.test(dataset_handler.train_bow_en, dataset_handler.train_bow_cn)
+        test_theta_en, test_theta_cn = self.test(dataset_handler.test_bow_en, dataset_handler.test_bow_cn)
+
         return train_theta_en, train_theta_cn, test_theta_en, test_theta_cn
```

## topmost/trainers/dynamic/DTM_trainer.py

```diff
@@ -1,14 +1,14 @@
 import gensim
 import numpy as np
 from gensim.models import ldaseqmodel
 from tqdm import tqdm
 import datetime
 from multiprocessing.pool import Pool
-from topmost.utils import static_utils
+from topmost.utils import basic_utils
 
 
 def work(arguments):
     model, docs = arguments
     theta_list = list()
     for doc in tqdm(docs):
         theta_list.append(model[doc])
@@ -110,15 +110,15 @@
         beta = beta / beta.sum(-1, keepdims=True)
         return beta
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
         top_words_list = list()
         for time in range(beta.shape[0]):
-            top_words = static_utils.print_topic_words(beta[time], vocab=self.dataset_handler.vocab, num_top=num_top)
+            top_words = basic_utils.print_topic_words(beta[time], vocab=self.dataset_handler.vocab, num_top=num_top)
             top_words_list.append(top_words)
         return top_words_list
 
     def export_theta(self):
         train_theta = self.get_theta()
         test_theta = self.test(self.dataset_handler.test_bow)
         return train_theta, test_theta
```

## topmost/trainers/dynamic/dynamic_trainer.py

```diff
@@ -1,19 +1,18 @@
 import numpy as np
 import torch
 from torch.optim.lr_scheduler import StepLR
-from collections import defaultdict
 from tqdm import tqdm
+from collections import defaultdict
 from topmost.utils import static_utils
 
 
 class DynamicTrainer:
-    def __init__(self, model, dataset_handler, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
+    def __init__(self, model, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
         self.model = model
-        self.dataset_handler = dataset_handler
         self.epochs = epochs
         self.learning_rate = learning_rate
         self.batch_size = batch_size
         self.lr_scheduler = lr_scheduler
         self.lr_step_size = lr_step_size
         self.log_interval = log_interval
 
@@ -26,43 +25,50 @@
         optimizer = torch.optim.Adam(**args_dict)
         return optimizer
 
     def make_lr_scheduler(self, optimizer):
         lr_scheduler = StepLR(optimizer, step_size=self.lr_step_size, gamma=0.5, verbose=False)
         return lr_scheduler
 
-    def train(self):
+    def fit_transform(self, dataset_handler, num_top_words=15, verbose=False):
+        self.train(dataset_handler, verbose)
+        top_words = self.export_top_words(dataset_handler.vocab, num_top_words)
+        train_theta = self.test(dataset_handler.train_bow, dataset_handler.train_times)
+
+        return top_words, train_theta
+
+    def train(self, dataset_handler, verbose=False):
         optimizer = self.make_optimizer()
 
         if self.lr_scheduler:
-            print("===>Warning: use lr_scheduler")
+            print("===>using lr_scheduler")
             lr_scheduler = self.make_lr_scheduler(optimizer)
 
-        data_size = len(self.dataset_handler.train_dataloader.dataset)
+        data_size = len(dataset_handler.train_dataloader.dataset)
 
         for epoch in tqdm(range(1, self.epochs + 1)):
             self.model.train()
             loss_rst_dict = defaultdict(float)
 
-            for batch_data in self.dataset_handler.train_dataloader:
+            for batch_data in dataset_handler.train_dataloader:
 
                 rst_dict = self.model(batch_data['bow'], batch_data['times'])
                 batch_loss = rst_dict['loss']
 
                 optimizer.zero_grad()
                 batch_loss.backward()
                 optimizer.step()
 
                 for key in rst_dict:
                     loss_rst_dict[key] += rst_dict[key] * len(batch_data)
 
             if self.lr_scheduler:
                 lr_scheduler.step()
 
-            if epoch % self.log_interval == 0:
+            if verbose and epoch % self.log_interval == 0:
                 output_log = f'Epoch: {epoch:03d}'
                 for key in loss_rst_dict:
                     output_log += f' {key}: {loss_rst_dict[key] / data_size :.3f}'
 
                 print(output_log)
 
     def test(self, bow, times):
@@ -80,20 +86,20 @@
         return theta
 
     def export_beta(self):
         self.model.eval()
         beta = self.model.get_beta().detach().cpu().numpy()
         return beta
 
-    def export_top_words(self, num_top_words=15):
+    def export_top_words(self, vocab, num_top_words=15):
         beta = self.export_beta()
         top_words_list = list()
         for time in range(beta.shape[0]):
             print(f"======= Time: {time} =======")
-            top_words = static_utils.print_topic_words(beta[time], vocab=self.dataset_handler.vocab, num_top_words=num_top_words)
+            top_words = static_utils.print_topic_words(beta[time], vocab, num_top_words)
             top_words_list.append(top_words)
         return top_words_list
 
-    def export_theta(self):
-        train_theta = self.test(self.dataset_handler.train_bow, self.dataset_handler.train_times)
-        test_theta = self.test(self.dataset_handler.test_bow, self.dataset_handler.test_times)
+    def export_theta(self, dataset_handler):
+        train_theta = self.test(dataset_handler.train_bow, dataset_handler.train_times)
+        test_theta = self.test(dataset_handler.test_bow, dataset_handler.test_times)
         return train_theta, test_theta
```

## topmost/trainers/hierarchical/HDP_trainer.py

```diff
@@ -1,14 +1,14 @@
 """
 https://radimrehurek.com/gensim/models/hdpmodel.html
 """
 
 import gensim
 from gensim.models import HdpModel
-from topmost.utils import static_utils
+from topmost.utils import basic_utils
 
 
 class HDPGensimTrainer:
     def __init__(self, dataset, max_chunks=None, max_time=None, chunksize=256, kappa=1.0, tau=64.0, K=15, T=150, alpha=1, gamma=1, eta=0.01, scale=1.0, var_converge=0.0001):
         self.dataset = dataset
         self.vocab_size = dataset.vocab_size
         self.max_chunks = max_chunks
@@ -58,14 +58,14 @@
         return theta
 
     def export_beta(self):
         return self.model.get_topics()
 
     def export_top_words(self, num_top_words=15):
         beta = self.export_beta()
-        top_words = static_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top_words=num_top_words)
+        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top_words=num_top_words)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset.train_bow)
         test_theta = self.test(self.dataset.test_bow)
         return train_theta, test_theta
```

## topmost/trainers/hierarchical/hierarchical_trainer.py

```diff
@@ -8,17 +8,16 @@
 
 # transform tensor list to numpy list
 def to_nparray(tensor_list):
     return np.asarray([item.detach().cpu().numpy() for item in tensor_list], dtype=object)
 
 
 class HierarchicalTrainer:
-    def __init__(self, model, dataset_handler, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
+    def __init__(self, model, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
         self.model = model
-        self.dataset_handler = dataset_handler
         self.epochs = epochs
         self.learning_rate = learning_rate
         self.batch_size = batch_size
         self.lr_scheduler = lr_scheduler
         self.lr_step_size = lr_step_size
         self.log_interval = log_interval
 
@@ -31,43 +30,50 @@
         optimizer = torch.optim.Adam(**args_dict)
         return optimizer
 
     def make_lr_scheduler(self, optimizer,):
         lr_scheduler = StepLR(optimizer, step_size=self.lr_step_size, gamma=0.5, verbose=True)
         return lr_scheduler
 
-    def train(self):
+    def fit_transform(self, dataset_handler, num_top_words=15, verbose=False):
+        self.train(dataset_handler, verbose)
+        top_words = self.export_top_words(dataset_handler.vocab, num_top_words)
+        train_theta = self.test(dataset_handler.train_data)
+
+        return top_words, train_theta
+
+    def train(self, dataset_handler, verbose=False):
         optimizer = self.make_optimizer()
 
         if self.lr_scheduler:
-            print("===>Warning: use lr_scheduler")
+            print("===>using lr_scheduler")
             lr_scheduler = self.make_lr_scheduler(optimizer)
 
-        data_size = len(self.dataset_handler.train_dataloader.dataset)
+        data_size = len(dataset_handler.train_dataloader.dataset)
 
         for epoch in tqdm(range(1, self.epochs + 1), leave=False):
             self.model.train()
             loss_rst_dict = defaultdict(float)
 
-            for batch_data in self.dataset_handler.train_dataloader:
+            for batch_data in dataset_handler.train_dataloader:
 
                 rst_dict = self.model(batch_data)
                 batch_loss = rst_dict['loss']
 
                 optimizer.zero_grad()
                 batch_loss.backward()
                 optimizer.step()
 
                 for key in rst_dict:
                     loss_rst_dict[key] += rst_dict[key] * len(batch_data)
 
             if self.lr_scheduler:
                 lr_scheduler.step()
 
-            if epoch % self.log_interval == 0:
+            if verbose and epoch % self.log_interval == 0:
                 output_log = f'Epoch: {epoch:03d}'
                 for key in loss_rst_dict:
                     output_log += f' {key}: {loss_rst_dict[key] / data_size :.3f}'
 
                 print(output_log)
 
     def test(self, bow):
@@ -84,30 +90,38 @@
                 batch_theta_list = self.model.get_theta(batch_input)
 
                 for layer_id in range(len(num_topics_list)):
                     theta_list[layer_id].extend(batch_theta_list[layer_id].cpu().numpy().tolist())
 
         theta = np.empty(len(num_topics_list), object)
         theta[:] = [np.asarray(item) for item in theta_list]
+
         return theta
 
     def export_phi(self):
         phi = to_nparray(self.model.get_phi_list())
         return phi
 
     def export_beta(self):
         beta_list = to_nparray(self.model.get_beta())
         return beta_list
 
-    def export_top_words(self, num_top_words=15):
+    def export_top_words(self, vocab, num_top_words=15, annotation=False):
         beta = self.export_beta()
         top_words_list = list()
+
         for layer in range(beta.shape[0]):
             print(f"======= Layer: {layer} number of topics: {beta[layer].shape[0]} =======")
-            top_words = static_utils.print_topic_words(beta[layer], vocab=self.dataset_handler.vocab, num_top_words=num_top_words)
-            top_words_list.append(top_words)
+            top_words = static_utils.print_topic_words(beta[layer], vocab, num_top_words=num_top_words)
+
+            if not annotation:
+                top_words_list.append(top_words)
+            else:
+                top_words_list.extend([f'L-{layer}_K-{k} {item}' for k, item in enumerate(top_words)])
+
         return top_words_list
 
-    def export_theta(self):
-        train_theta = self.test(self.dataset_handler.train_bow)
-        test_theta = self.test(self.dataset_handler.test_bow)
+    def export_theta(self, dataset_handler):
+        train_theta = self.test(dataset_handler.train_data)
+        test_theta = self.test(dataset_handler.test_data)
+
         return train_theta, test_theta
```

## Comparing `topmost-0.0.1.dist-info/LICENSE` & `topmost-0.0.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `topmost-0.0.1.dist-info/RECORD` & `topmost-0.0.2.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,26 +1,31 @@
 topmost/__init__.py,sha256=d_oJW-0EVhW7rLcqTOEWt4lBx7MDVQxp_JNMQtCUu5w,117
-topmost/data/__init__.py,sha256=NOeRnXZnAf_LHTAq0MHpvoc96NyAlmR-I9ys2tozxsg,223
-topmost/data/basic_dataset_handler.py,sha256=47FtuiCUbhAk9vAHIGWNtFAN4DzsWM_SRofe3yL352s,2017
+topmost/data/__init__.py,sha256=q3j4dEmTa1cU3NhoOGmyG6z8H3ztOmLvNq5sV6TJBGg,276
+topmost/data/basic_dataset_handler.py,sha256=CxGrxJxgAfp6532MnuXonmUkFMicupxkW5zJByva0CY,3752
 topmost/data/crosslingual_dataset_handler.py,sha256=tZjqnu_1Rc_t7YJlLYZdKnFpzsvQjguxxU-lQabWSig,5009
-topmost/data/download.py,sha256=LV4tu1pr2JENMauIldvqs7h1tI23XRgb8jxfG7rWZZY,624
+topmost/data/download.py,sha256=UXHuPSbOGh3oR3tOwhFzrfAFa2SPJQTmLTzOOASs3zc,827
 topmost/data/download_20ng.py,sha256=j3FIBSfd8wQ0DLIRqRCJpx-PWID59BpWbfGvkxZMc4Y,1685
 topmost/data/dynamic_dataset_handler.py,sha256=prU6mFdfhNGXhRm9eapNWOobp2dXqdu3UIAak798abU,3608
-topmost/data/file_utils.py,sha256=Wr838jG0M1ZRygwXW0qWzvymwMIxK026Vpd0Vhdm1_E,1223
-topmost/evaluations/__init__.py,sha256=cjUyCc39yf9_wrdjF04oiM7wvDS6NELQBVb3pvVoPNg,351
-topmost/evaluations/classification.py,sha256=679HYTHq8Irr5ih99S5qcqnP9XwLY3sVnq7Nlefl_SU,2004
-topmost/evaluations/clustering.py,sha256=axMh1k0Whdlbj3iv_1kvteSLPgyVGPwzaTo7cPk4Vgw,1306
+topmost/data/file_utils.py,sha256=NtaPLWk7e-fKa6QIgol2yNdUIewyGfMifY62oKnYJxw,952
+topmost/evaluations/__init__.py,sha256=1CfWWc6ormJItxopdcSUZYSQSck2lzCcRvUpZwmwGmU,520
+topmost/evaluations/build_hierarchy.py,sha256=1x5E6fIR0vzhUqkDA-SBvCHcLsixCTjK-jHFeUASyfM,1104
+topmost/evaluations/classification.py,sha256=_fk3_jnjTCZJa9tmUxrSenCwMTXOgGSu5MJmkoB15tw,1986
+topmost/evaluations/clustering.py,sha256=s7UT1_m8DrX-BzFGGhQRdnK_zBPWwj_kNLg6IK-zZ-o,1297
+topmost/evaluations/hierarchy_quality.py,sha256=GMKZyhPYMfmBua41DyzQKaOIpuv-eKsqlq7q4NTWqc0,9263
 topmost/evaluations/topic_coherence.py,sha256=Kk6EojxO9jLU-RIlt42o1lRrN3EpP2A8oGS9oszLlho,1413
-topmost/evaluations/topic_diversity.py,sha256=4OeRvP2qAfkW8R4FhgKu_MhMJnLZi4GgxQeIZCfQgAo,694
-topmost/models/Encoder.py,sha256=TSMh0oOpAUz216A3ZHq-XbqXxda1jCkEK2W6CmuL8Ys,1257
-topmost/models/__init__.py,sha256=8U59M9yhauBGcjDctCcRKg7Ax7byRUuBGq9LGAyX4S8,427
+topmost/evaluations/topic_diversity.py,sha256=E8ooUHHWwWV2Qj2Iiz501LfI5pYtbXoVRY2Z79Wtc8E,686
+topmost/models/Encoder copy.py,sha256=Cb1FagwiuCKeDWVRngQKeMnmJ4Yp5kDJ4yYD56hj4Ic,1373
+topmost/models/Encoder.py,sha256=PDcZI_qf2PyATaEhaj8D9CZdWEf8VbVWmn0E6gj1a-A,1326
+topmost/models/__init__.py,sha256=m4l_sXkE2bFwcLZ0zDedK4DyaA2ukHhIgjNiTpCyxvM,512
+topmost/models/basic/CombinedTM.py,sha256=mzRVnwPpkJD6VsXuporkEgPhxlMaEoUvPtJtAYQYFNQ,3917
 topmost/models/basic/DecTM.py,sha256=dYnkVd8YPJ9oVjJR6C43neqfEwFbXP3RWlAn6wkC2tw,3758
 topmost/models/basic/ETM.py,sha256=IecGChTLYJDll--JKIA5pEUB80kBEEeX3uKgZKjbvaE,2512
 topmost/models/basic/ProdLDA.py,sha256=Z8hCY48VktzdFoYMXJqQfUa-Y5JG_oKieJtaqv-vJ6I,3646
 topmost/models/basic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+topmost/models/basic/ECRTM/CombinedECRTM.py,sha256=WzsSQJ0Lz19kpavKS8nJJcNeUxv7j-J_kV1HjrxTHhA,4619
 topmost/models/basic/ECRTM/ECR.py,sha256=AYA6F1DpQcMAkGp-CUqZr7WBJEAqEaVPrOvlVsavVU4,1503
 topmost/models/basic/ECRTM/ECRTM.py,sha256=jSv3k417Y-smWwI-Y4Ay10v0qL0H42URLPDazOWMPpk,4506
 topmost/models/basic/ECRTM/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/basic/NSTM/NSTM.py,sha256=MKa0EuqQZ6WKGnGklDn4h5Sr080KPB5wnpAqC7R59hs,2253
 topmost/models/basic/NSTM/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/basic/NSTM/auto_diff_sinkhorn.py,sha256=g-Ooiy0mpbQwMeVmfDMPcsRc7QGuwkPgZl6Nkg4aUc8,850
 topmost/models/basic/TSCTM/TSC.py,sha256=WnZS5_G2bxOb4XU1YCCWrVjvxnIuKPZohMnkkCRJ4Ro,2941
@@ -39,33 +44,43 @@
 topmost/models/hierarchical/HyperMiner/HyperMiner.py,sha256=aBDXZgV2BG7KR6sNGcnSYRXfmIvfaTi3JLImo01exts,5234
 topmost/models/hierarchical/HyperMiner/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/hierarchical/HyperMiner/manifolds/__init__.py,sha256=Ch1JFMW3NUrifGjGPoWR3IAusBjQo6tqHnPpKAJINw8,107
 topmost/models/hierarchical/HyperMiner/manifolds/base.py,sha256=QpmqCfJI1B1eEfNwPxuuFjSaQI5nQY976a0N6xJhZm4,6632
 topmost/models/hierarchical/HyperMiner/manifolds/euclidean.py,sha256=_hAAEzaXSJYsr_SXglGHMJqYvqoB83s_SISgEeksjpA,1386
 topmost/models/hierarchical/HyperMiner/manifolds/math_util.py,sha256=Is5U0bWo5w1Xi-fty2KJ97--eYdlh7RQN-b3X4HEmeY,5117
 topmost/models/hierarchical/HyperMiner/manifolds/poincare.py,sha256=a6DtPcl88vvKHE7W5EupW5wVaUynKraZy2eKK4DO8KU,4403
+topmost/models/hierarchical/ProGBN/ProGBN.py,sha256=66-7k12K8EoGR7OjCFWtV3JGYGpTx8jEYZ0oLcx8kPg,10196
+topmost/models/hierarchical/ProGBN/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+topmost/models/hierarchical/ProGBN/utils.py,sha256=yO_j3eBXI7MakXj9ZYpymc8PBunnurCmqGUpDOM8k28,4957
 topmost/models/hierarchical/SawETM/SawETM.py,sha256=7FcnvPq_C8qZzNCfk-scx-kZe5mp0oh74jJZdtoVK6o,7821
 topmost/models/hierarchical/SawETM/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/hierarchical/SawETM/block.py,sha256=zfOCpW0MZczAGma-x6YHA5juFwwxSGy9FCWs2Z_T0NM,1392
+topmost/models/hierarchical/TraCo/CDDecoder.py,sha256=YzDlmXqtn6pD6zfQwC9qgHnyu1zLIFTuTJWyi_Fxa7A,1900
+topmost/models/hierarchical/TraCo/TPD.py,sha256=NS0-Jqs7MtVJaNW3iy7QS2BHm_9Y5-iOwHJWUp9xkK4,1952
+topmost/models/hierarchical/TraCo/TraCo.py,sha256=g1in457vCQLWvFlVMwZp2MoYJT1CijasAe3zjhcYQho,4262
+topmost/models/hierarchical/TraCo/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+topmost/models/hierarchical/TraCo/utils.py,sha256=lwj-BAv7NssHVRqoVm47DuD3BJV79-v9EmMjtlDJ15g,485
 topmost/preprocessing/__init__.py,sha256=7XpnLXPEubaZurrigk9l28fmZalMj6VXJKQ7xqsCU_g,41
-topmost/preprocessing/preprocessing.py,sha256=o6KFqfTnrztINBddh4hUaH8TP_Mnd_QUMBVrTEteoYI,12014
-topmost/trainers/__init__.py,sha256=vGXFcUdOR-9CJZt7d4Sprq1dDulWsPpQTl0jEwlvQYw,528
-topmost/trainers/basic/BERTopic_trainer.py,sha256=KXgOyznYndhAqlYaUHephBq8wcffk9g--qxRI7Vp6K8,963
-topmost/trainers/basic/LDA_trainer.py,sha256=M8U20b7A5WUGDH1YWH13P3DkwLkJ4rUi2Jyy4-ow12A,2440
-topmost/trainers/basic/NMF_trainer.py,sha256=nokrOSCaqzyEoacMzEQ-dUwNn0ivLJfsub4lkTN-Ow0,2424
+topmost/preprocessing/preprocessing.py,sha256=qtvYV9Zi9rBdRoNz2L51gIrrqE_OrWjRYC8Pe_gCg7M,12646
+topmost/trainers/__init__.py,sha256=BzvDldLbY87SSfJiBUc6b10oG2bw3vpL7BSS2ZKqBDo,580
+topmost/trainers/basic/BERTopic_trainer.py,sha256=0loBNOoBp2a6Eh3ntOA_NQn3_cN6f0F9AV0pkZps8TM,920
+topmost/trainers/basic/LDA_trainer.py,sha256=9q3dtPogwSVEFwo-Yl1eAx9qVd1A8YejKaXOHk30ygU,2437
+topmost/trainers/basic/NMF_trainer.py,sha256=ExA_hR68tGInR7OCAi3FBCR5X_LOMwv4TxjNnw8P6jk,2421
 topmost/trainers/basic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/trainers/basic/basic_trainer.py,sha256=-texkhU_Ac8Rk_9VNc86NafHUvXXcfPPMBXTBqAGdes,3192
+topmost/trainers/basic/basic_contextual_trainer.py,sha256=zRw2tS6VHAw2hr0rotw0qJNcObDI3gGirWND0MAFJis,3214
+topmost/trainers/basic/basic_trainer.py,sha256=eFQC6OFBEjZKfvFb4bl3uv5USQxIqDj04Bh2wJt1VF8,3551
 topmost/trainers/crosslingual/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/trainers/crosslingual/crosslingual_trainer.py,sha256=S75SCtdjKzIm6VYvXvfkO-B_m-wNFqcsPSNoZTqjxbw,4038
-topmost/trainers/dynamic/DTM_trainer.py,sha256=fC8yO73k6jwJdXgubJBV4buWd8fTMzlCLhbU5ry2vXo,4169
+topmost/trainers/crosslingual/crosslingual_trainer.py,sha256=NVOVEFYlalNlCtXekr_ftQ28ySRScU8yISNykGMAzgs,4428
+topmost/trainers/dynamic/DTM_trainer.py,sha256=KDk_oA2DBIRd0wl0vRbfUJeU53kRJAor_srpngMJuHI,4167
 topmost/trainers/dynamic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/trainers/dynamic/dynamic_trainer.py,sha256=w4Zs2CVhWW7WrkEy_f597p_UJ3hSOA4YyCkdql3iLRA,3456
-topmost/trainers/hierarchical/HDP_trainer.py,sha256=ZIxmHgamyYVAbd-JSMNxss_RERIfT9IRmUfk7KXr7_A,2375
+topmost/trainers/dynamic/dynamic_trainer.py,sha256=YJg9ubeoJHdTvo6txaJio93ALfH9qriZ0-3r9WGE_gg,3714
+topmost/trainers/hierarchical/HDP_trainer.py,sha256=G0iTBDNgRH24YJbBEAbBafDY8Hq_qD0T1vfK2l1juEM,2373
 topmost/trainers/hierarchical/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/trainers/hierarchical/hierarchical_trainer.py,sha256=eftui6_4K-4hZ-0znWN-VmodtL1Ol8wn-TfKLM5oMuQ,3967
+topmost/trainers/hierarchical/hierarchical_trainer.py,sha256=jjiboubbbt_H1NMZGsYJGXHXO51zKHqN1e9PzqaVHss,4402
 topmost/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+topmost/utils/basic_utils.py,sha256=Zavmrraj0TNvVmkTzOqSpCMOz5CF4-J0nMsuWalTi88,390
 topmost/utils/static_utils.py,sha256=Zavmrraj0TNvVmkTzOqSpCMOz5CF4-J0nMsuWalTi88,390
-topmost-0.0.1.dist-info/LICENSE,sha256=yVuuHRzgI17MzTVgt3LsHvuX80innw--CmNPDCzO_iw,11358
-topmost-0.0.1.dist-info/METADATA,sha256=EIVGt8Z5g0tqOF-yqJ-TU2g8NDWP4jU6jPu6tASWmKs,1081
-topmost-0.0.1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-topmost-0.0.1.dist-info/top_level.txt,sha256=LGxnB073Vb97ZHW3Iz16d0q991aBULtbPFpwmYKqVME,8
-topmost-0.0.1.dist-info/RECORD,,
+topmost-0.0.2.dist-info/LICENSE,sha256=yVuuHRzgI17MzTVgt3LsHvuX80innw--CmNPDCzO_iw,11358
+topmost-0.0.2.dist-info/METADATA,sha256=XxRjx10EtCOOQ1imMxUTBtNwLjDytFhHqX0BZMwHEQA,16609
+topmost-0.0.2.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+topmost-0.0.2.dist-info/top_level.txt,sha256=LGxnB073Vb97ZHW3Iz16d0q991aBULtbPFpwmYKqVME,8
+topmost-0.0.2.dist-info/RECORD,,
```

